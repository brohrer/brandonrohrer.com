<rss version="2.0">
<channel>
<title>Brandon Rohrer</title>
<link>https://www.brandonrohrer.com</link>
<description>Brandon Rohrer's blog</description>

  <item>
    <title>
    Transformers from Scratch 6/ðŸ§µ
    </title>
    <link>
    https://www.brandonrohrer.com/transformers#second_order_skips
    </link>
    <pubDate>
    Wed, 20 Aug 2025 06:34:00 EDT
    </pubDate>
    <guid>
    https://www.brandonrohrer.com/transformers#second_order_skips
    </guid>
    <description><![CDATA[
      <h3 id="second_order_skips">Second order sequence model with skips</h3>
        <p>
          A second order model works well when we only have to look back
          two words to decide what word comes next. What about when we
          have to look back further? Imagine we are building yet another
          language model. This one only has to represent two sentences,
          each equally likely to occur.
          <ul>
            <li>
              <em>Check the program log and find out whether it ran please.
              </em>
            </li>
            <li>
              <em>Check the battery log and find out whether it ran down
              please.</em>
            </li>
          </ul>
        </p>
        <p>
          In this example, in order to determine which word should come after
          <em>ran</em>, we would have to look back 8 words into the past.
          If we want to improve on our second order language model,
          we can of course consider third- and higher order models.
          However, with a significant vocabulary size this takes
          a combination of creativity and brute force
          to execute. A naive implementation of an eighth order model
          would have <em>N</em>^8 rows, a ridiculous number for any
          reasonable vocubulary.
        </p>
        <p>
          Instead, we can do something sly and make a second order model,
          but consider the combinations of the most recent word with
          each of the words that came before. It's still second order,
          because we're only considering two words at a time, but it allows
          us to reach back further and capture <strong>long range
          dependencies</strong>. The difference between this
          second-order-with-skips and a full umpteenth-order model is that
          we discard most of the word order information and
          combinations of preceeeding words. What remains is still pretty
          powerful.
        </p>
        <p>
          Markov chains fail us entirely now, but we can still represent
          the link between each pair of preceding words and the words
          that follow. Here we've dispensed with numerical weights, and
          instead are showing only the arrows associated with non-zero weights.
          Larger weights are shown with heavier lines.
        </p>
        <p style="text-align:center;">
          <img title="Second order with skips feature voting"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/feature_voting.png"
            alt="Second order with skips feature voting"
            style="height: 350px;">
        </p>
        <p>
          Here's what it might look like in a transition matrix.
        </p>
        <p style="text-align:center;">
          <img title="Second order with skips transition matrix"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/transition_matrix_second_order_skips.png"
            alt="Second order with skips transition matrix"
            style="height: 350px;">
        </p>
        <p>
          This view only shows the rows relevant to predicting the word
          that comes after <em>ran</em>. It shows instances where the most
          recent word (<em>ran</em>) is preceded by each of the other
          words in the vocabulary. Only the relevant values are shown.
          All the empty cells are zeros. 
        </p>
        <p>
          The first thing that becomes apparent is that, when trying to
          predict the word that comes after <em>ran</em>, we no longer
          look at just one line, but rather a whole set of them.
          We've moved out of the Markov realm now. Each row no longer
          represents the state of the sequence at a particular point.
          Instead, each row represents one of many <strong>features</strong>
          that may describe the sequence at a particular point. The
          combination of the most recent word with each of the words
          that came before makes for a collection of applicable rows,
          maybe a large collection. Because of this change in meaning,
          each value in the matrix no longer represents a probability,
          but rather a vote. Votes will be summed and compared to determine
          next word predictions.
        </p>
        <p>
          The next thing that becomes apparent is that most of the features
          don't matter. Most of the words appear in both sentences, and
          so the fact that they have been seen is of no help in predicting
          what comes next. They all have a value of .5. 
          The only two exceptions are <em>battery</em> and <em>program</em>.
          They have some 1 and 0 weights associated with
          the two cases we're trying to distinguish.
          The feature <em>battery, ran</em> indicates that <em>ran</em> was
          the most recent word and that <em>battery</em> occurred somewhere
          earlier in the sentence. This feature has a weight of 1 associated
          with <em>down</em> and a weight of 0 associated with <em>please</em>.
          Similarly, the feature <em>program, ran</em> has the opposite set
          of weights. This structure shows that it is the presence of these
          two words earlier in the sentence that is decisive in predicting
          which word comes next.
        </p>
        <p>
          To convert this set of word-pair features into a next word estimate,
          the values of all the relevant rows need to be summed.
          Adding down the column, the sequence
          <em>Check the program log and find out whether it ran</em>
          generates sums of 0 for all the words, except a 4 for
          <em>down</em> and a 5 for <em>please</em>. The sequence
          <em>Check the battery log and find out whether it ran</em>
          does the same, except with a 5 for
          <em>down</em> and a 4 for <em>please</em>. By choosing the word
          with the highest vote total as the next word prediction,
          this model gets us the right answer, despite having an
          eight word deep dependency.
        </p>

      ]]></description>
  </item>

  <item>
    <title>
    Transformers from Scratch 5/ðŸ§µ
    </title>
    <link>
    https://www.brandonrohrer.com/transformers#second_order
    </link>
    <pubDate>
    Tue, 19 Aug 2025 20:34:00 EDT
    </pubDate>
    <guid>
    https://www.brandonrohrer.com/transformers#second_order
    </guid>
    <description><![CDATA[
        <h3 id="second_order">Second order sequence model</h3>
        <p>
          Predicting the next word based on only the current word is
          hard. That's like predicting the rest of a tune after being
          given just the first note. Our chances are a lot better if
          we can at least get two notes to go on.
        </p>
        <p>
          We can see how this works in another toy language model
          for our computer commands. We expect that this one
          will only ever see two sentences, in a 40/60 proportion.
          <ul>
            <li>
              <em>Check whether the battery ran down please.</em>
            </li>
            <li>
              <em>Check whether the program ran please.</em>
            </li>
          </ul>
          A Markov chain illustrates a first order model for this.
        </p>
        <p style="text-align:center;">
          <img title="Another first order Markov chain transition model"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/markov_chain_2.png"
            alt="Another first order Markov chain transition model"
            style="height: 250px;">
        </p>
        <p>
          Here we can see that if our model looked at the two most recent
          words, instead of just one, that it could do a better job. When it
          encounters <em>battery ran</em>, it knows that the next word
          will be <em>down</em>, and when it sees <em>program ran</em>
          the next word will be <em>please</em>. This eliminates one of
          the branches in the model, reducing uncertainty and increasing
          confidence.
          Looking back two words turns this into a second order Markov model.
          It gives more context on which to base next word predictions.
          Second order Markov chains are more challenging to
          draw, but here are the connections that demonstrate their value.
        </p>
        <p style="text-align:center;">
          <img title="Second order Markov chain"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/markov_chain_second_order.png"
            alt="Second order Markov chain"
            style="height:250px;">
        </p>
        <p>
          To highlight the difference between the two,
          here is the first order transition matrix,
        </p>
        <p style="text-align:center;">
          <img title="Another first order transition matrix"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/transition_matrix_first_order_2.png"
            alt="Another first order transition matrix"
            style="height: 350px;">
        </p>
        <p>
          and here is the second order transition matrix.
        </p>
        <p style="text-align:center;">
          <img title="Second order transition matrix"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/transition_matrix_second_order.png"
            alt="Second order transition matrix"
            style="height: 350px;">
        </p>
        <p>
          Notice how the second order matrix has a separate row for every
          combination of words (most of which are not shown here). That
          means that if we start with a vocabulary size of <em>N</em>
          then the transition matrix has <em>N</em>^2 rows.
        </p>
        <p>
          What this
          buys us is more confidence. There are more ones and fewer
          fractions in the second order model. There's only one row
          with fractions in it, one branch in our model. Intuitively,
          looking at two words instead of just one gives more context,
          more information on which to base a next word guess.
        </p>

      ]]></description>
  </item>

  <item>
    <title>
    How to start publishing an RSS feed
    </title>
    <link>
    https://www.brandonrohrer.com/rss.html
    </link>
    <pubDate>
    Mon, 18 Aug 2025 20:34:00 EDT
    </pubDate>
    <guid>
    https://www.brandonrohrer.com/rss.html
    </guid>
    <description><![CDATA[
<p>
Before Twitter, before LinkedIn, before Facebook, there was
<a href="https://en.wikipedia.org/wiki/RSS">RSS</a>.
Really Simple Syndication is the photocopied 'zine of microblogging.
If there were social media in Mad Max, it would have been RSS. It's
totally outside any centralized control, cheap, gritty, and punk af. 
</p>

<h2><a id="Reading-RSS"></a><a href="#Reading-RSS">Reading RSS</a></h2>

<p>
The toughest thing to get used to is that there is no central platform.
RSS is just a bunch of people creating RSS-formatted files and posting them
on the internet. The burden of assembling them into a reading list falls
on the reader. Thankfully there <strong>aggregators</strong>, helpful programs that regularly
check those RSS files for changes and lay them out for you in a feed.
</p>

<p>
I use <a href="https://feedly.com">Feedly</a> daily, and enjoy
<a href="https://www.inoreader.com">Inoreader</a> too. I've also heard that 
<a href="https://feeder.co">Feeder</a> and 
<a href="https://newsblur.com">NewsBlur</a> are solid options. There are plenty
of others, some with niche functionality. The cool part is that there isn't
a "main" one or a home base. They're all their own thing. All they do
is gather up changes and put them in a list for you.
</p>

<p>
To subscribe to someone's feed, you'll need to get the URL. These are
frequently posted on their blog under an "RSS feed" link or the
RSS logo.
</p>

<p>
<img alt="The RSS logo, a dot at the center of two quarter-circles, 
giving the appearance of waves radiating out from a central point
" src="https://upload.wikimedia.org/wikipedia/en/4/43/Feed-icon.svg">
</p>

<p>
The URL is to an .xml file. For my blog it looks like
</p>

<p>
<pre>
https://brandonrohrer.com/feed.xml
</pre>
</p>

<p>
For an example feed created for this post, it looks like
</p>

<p>
<pre>
https://raw.githubusercontent.com/brohrer/blog/refs/heads/main/code/example_feed.xml
</pre>
</p>

<p>
This is the key to subscribing. Copy the URL and paste it into the
field for 
"Add channel" or "Follow feeds" or whatever other name your aggregator uses.
And from then on your aggregator will revisit that URL occasionally,
checking for new content, and add it to your feed.
</p>

<p>
<figure>
  <img title="A Follow Sources dialog from Feedly" alt="A Follow Sources dialog page, with a field for pasting a feed URL
and below that a list of feeds that match the URL
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/rss/follow_sources.png">
  <figcaption>A Follow Sources dialog from Feedly</figcaption>
</figure>
</p>

<p>
<figure>
  <img title="A channel overview on Feedly" alt="An overview of the feed, showing channel title and a thumbnail
view of the first post
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/rss/feed_overview.png">
  <figcaption>A channel overview on Feedly</figcaption>
</figure>
</p>

<p>
<figure>
  <img title="An example post, viewed on Feedly" alt="An example post with the title Post Content and some text reading 
This can be any html.
followed by a vintage drawing of Christopher Robin reading to 
Winnie the Pooh, who is stuck in a hole.
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/rss/example_post.png">
  <figcaption>An example post, viewed on Feedly</figcaption>
</figure>
</p>

<p>
Most aggregators also have a way to explore the more popular feeds.
</p>

<p>
<figure>
  <img title="Some popular feed topics, offered on Feedly" alt="A list of 15 industries and a handful skills as topics to search for feeds
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/rss/feed_topics.png">
  <figcaption>Some popular feed topics, offered on Feedly</figcaption>
</figure>
</p>

<h2><a id="Writing-RSS"></a><a href="#Writing-RSS">Writing RSS</a></h2>

<p>
If the process of following RSS feeds seems duct-tape-and-bailing-wire,
you'll love writing them.
</p>

<h3><a id="1.-Find-a-place-to-host-it"></a><a href="#1.-Find-a-place-to-host-it">1. Find a place to host it</a></h3>

<p>
Aggregators need to be able to find your feed .xml file on the internet.
Any place you can upload a text file and share it with the world should do.
One free option is <a href="https://www.github.com">GitHub</a> if that's a tool
you're familiar with. Here's 
<a href="https://github.com/brohrer/blog/blob/main/code/example_feed.xml">the .xml file for the example feed
</a>
which is on GitHub.
</p>

<p>
<pre>
&lt;rss version="2.0"&gt;
  &lt;channel&gt;
    &lt;title&gt;Your Feed&lt;/title&gt;
    &lt;link&gt;https://www.brandonrohrer.com&lt;/link&gt;
    &lt;description&gt;Your Blog's name&lt;/description&gt; <br>
    &lt;item&gt;
      &lt;title&gt;Blog title&lt;/title&gt;
      &lt;link&gt;https://www.brandonrohrer.com/rss.html&lt;/link&gt;
      &lt;pubDate&gt;Sat, 16 Aug 2025 12:31:00 EDT&lt;/pubDate&gt;
      &lt;guid isPermaLink="false"&gt;https://www.brandonrohrer.com/rss.html.04&lt;/guid&gt;
      &lt;description&gt;&lt;![CDATA[
        &lt;h1&gt;Post content&lt;/h1&gt;
        &lt;p&gt;
          This can be any html.
        &lt;/p&gt;
        &lt;img src="https://upload.wikimedia.org/wikipedia/commons/9/97/Winnie-the-Pooh_45-1.png"&gt;
      ]]&gt;&lt;/description&gt;
    &lt;/item&gt; <br>
  &lt;/channel&gt;
&lt;/rss&gt;
</pre>
</p>

<p>
You can copy this directly into your own feed .xml and modify it. 
A trick to remember with GitHub is that the link to your feed will actually
be the "raw" link, which is available from the icon on the right side
of the screen when looking at the file in GitHub.
</p>

<p>
There are two major sections, the <code>channel</code> information at the top,
then information for each <code>item</code> below that.
</p>

<h3><a id="2.-Add-channel-information"></a><a href="#2.-Add-channel-information">2. Add channel information</a></h3>

<ul>
<li> <code>title</code> is the channel name. It can be anything you want. Itâ€™s what
people will see when they pull up your channel in their aggregator.</li>
<li> <code>link</code> is a website associated with your channel. For me, it's the
landing page of my blog.</li>
<li> <code>description</code> is typically a one-line explanation of what readers can
expect to see in your feed.</li>
</ul>

<p>
There are <a href="https://www.rssboard.org/rss-specification#optionalChannelElements">lots of other elements
</a>
you can add here if you like, but these are the required ones.
</p>

<h3><a id="3.-Add-item-information"></a><a href="#3.-Add-item-information">3. Add item information</a></h3>

<p>
Once you have the channel info in place you can add an item.
An item needs a few basic pieces of information.
</p>

<ul>
<li> <code>title</code> is the name of the particular post. </li>
<li> <code>link</code> is a URL associated with it.</li>
<li> <code>pubDate</code> is <a href="https://whitep4nth3r.com/blog/how-to-format-dates-for-rss-feeds-rfc-822/#valid-rfc-822-date-format">a date in the format of
</a>
<code>Fri, 16 Aug 2025 19:31:00 EDT</code>. It shows up at the top of a post as
the publication date.</li>
<li> <code>guid</code> (globally unique identifier) is a string that aggregators can use
as an ID for this post. I find it useful for when I update the content
of the post and I want aggregators to re-load it on their next pass.
Changing the guid signals to the aggregator that the post needs to be
re-loaded. By default, RSS assumes that this is a permanent link to the
website being promoted by this post item. If you include the argument
<code>isPermaLink="false"</code> that signals that this isn't intended to be a URL, just
an identifying string.</li>
<li> <code>description</code> is the body of the post. It can be a one-line teaser
for the linked content or it can be an entire novel. Everything
between the <code>&lt;![CDATA[</code> and <code>]]&gt;</code> delimiters will be interpreted as straight
html, which is super useful if you want to do pretty formatting or
include images. Not all aggregators will interpret all html tags
(for instance <code>&lt;script&gt;</code>
<a href="https://validator.w3.org/feed/docs/warning/SecurityRisk.html">is likely to get skipped</a>
for security reasons),
but any html that gives the aggregator pause will usually just be skipped over.</li>
</ul>

<p>
There are a number of
<a href="https://www.rssboard.org/rss-specification#hrelementsOfLtitemgt">other item elements</a>
you an add if you wish, but this is the subset I've found most useful.
</p>

<p>
You can add as many items as you like. Just repeat the <code>&lt;item&gt;</code> section.
</p>

<p>
<pre>...
    &lt;item&gt;
      &lt;title&gt;First post&lt;/title&gt;
      ...
    &lt;/item&gt;
    ...
    &lt;item&gt;
      &lt;title&gt;Second post&lt;/title&gt;
      ...
    &lt;/item&gt;
   ...
    &lt;item&gt;
      &lt;title&gt;Third post&lt;/title&gt;
      ...
    &lt;/item&gt;
    ...
</pre>
</p>

<p>
And that's it. You've created an RSS feed. Every time you edit your .xml
file, you've updated your feed.
</p>

<p>
Debugging feed .xml files can be fiddly. There are a lot of details that
have to be just so, and it can take a loooong time to
wait for the aggregrator to re-scan so that you can check the results.
A cool thing I discovered while writing this is that there are RSS
validation services like 
<a href="https://validator.w3.org/feed/">this one</a> and
<a href="https://www.rssboard.org/rss-validator/check.cgi">this one</a>
where you can
enter the contents of your feed file or the URL for your feed,
and it will check your feed for errors
right away. It revealed several imperfections in my own feed that I 
was oblivious to.
</p>

<h2><a id="RSS-is-different-than-other-social-media"></a><a href="#RSS-is-different-than-other-social-media">RSS is different than other social media</a></h2>

<h3><a id="Things-that-you-give-up"></a><a href="#Things-that-you-give-up">Things that you give up</a></h3>

<p>
When you send an RSS post out into the Internet, you can't tell whether one
person sees it or a thousand. You have no <strong>analytics</strong>, no likes, hearts,
or stars, no engagement dopamine. There is also no <strong>interaction</strong> with your
your readers, no comment section, Q and A, or quote posts.
There is also no <strong>algorithm</strong> you can game for boosting. Posts
just show up in the order they are published.
</p>

<h3><a id="Things-you-get"></a><a href="#Things-you-get">Things you get</a></h3>

<p>
In exchange for all that, you get a more <strong>deliberate experience</strong> of reading and writing.
There is more friction in publishing and following a feed than of downloading
an app and snapping a picture of your latte.
RSS requires intention, and so invites more thought.
It makes doomscrolling all but impossible.
Related to this, posts are often longer and more thought out than
the Tweets of yore. In my experience there are
<strong>more quirks and rabbit holes</strong>.
Most importantly, you also get <strong>control</strong> over what you can publish and
in what format.  You have independence from any rules or policy decisions
that might be imposed by a central authority.
There is no one who can boost it, bury it, or shut it down.
It is an indestructible distribution channel.
</p>

<h3><a id="A-note-about-Atom"></a><a href="#A-note-about-Atom">A note about Atom</a></h3>

<p>
You don't have to spend long in the world of RSS before you start hearing about
Atom. Atom is a very similar format that serves a very similar purpose.
It was backed by Google and for a while there was a great deal of contention
between proponents of RSS and Atom, which led to a lot of intense...um...discussion. 
<a href="https://danielmiessler.com/blog/atom-rss-why-we-should-just-call-them-feeds-instead-of-rss-feeds">Atom is better</a>
in the sense that it resolved several points of ambiguous behavior and
unappealing flaws of RSS, but in retrospect, the "reader wars" were probably
more distracting than productive and left the community a little fragmented.
</p>

<p>
Atom still exists. Plenty of people still use it, although from what
I gather RSS is more popular. Most aggregators can read both RSS and Atom just fine.
The shortcomings of RSS are not anything that is likely to bother any but
the geekiest users. I leave it as an exercise to the reader to decide if it
matters enough to spend any cognitive cycles on.
</p>

      ]]></description>
  </item>

  <item>
    <title>
    Transformers from Scratch 4/ðŸ§µ
    </title>
    <link>
    https://www.brandonrohrer.com/transformers#markov_chain
    </link>
    <pubDate>
    Fri, 15 Aug 2025 19:01:00 EDT
    </pubDate>
    <guid isPermaLink="false">
    2025081502
    </guid>
    <description><![CDATA[
        <h3>First order sequence model</h3>
        <p>
          We can set aside matrices for a minute and get back to what
          we really care about, sequences of words. Imagine that as we
          start to develop our natural language computer interface
          we want to handle just three possible commands:
        </p>
          <ul>
            <li>
              <em>Show me my directories please</em>.
            </li>
            <li>
              <em>Show me my files please</em>.
            </li>
            <li>
              <em>Show me my photos please</em>.
            </li>
          </ul>
        <p>
          Our vocabulary size is now seven:<br>
          {<em>directories, files, me, my, photos, please, show</em>}.
        </p>
        <p>
          One useful way to represent sequences is with a transition model.
          For every word in the vocabulary, it shows what the next word
          is likely to be. If users ask about photos half
          the time, files 30% of the time, and directories the rest
          of the time, the transition model will look like this.
          The sum of the transitions away from any word will always add up
          to one.
        </p>
          <img title="Markov chain transition model"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/markov_chain.png"
            alt="Markov chain transition model"
            style="height: 250px;">
        <p>
          This particular transition model is called a
          <strong>Markov chain</strong>,
          because it satisfies the
          <a href="https://en.wikipedia.org/wiki/Markov_property">
            Markov property</a>
          that the probabilities for the next word depend only on
          recent words. More specifically, it is a first order
          Markov model because it only looks at the single most recent word.
          If it considered the two most recent words it would be
          a second order Markov model.
        </p>
        <p>
          Our break from matrices is over. It turns out that
          Markov chains can be expressed conveniently in matrix form.
          Using the same indexing scheme that we used when creating one-hot
          vectors, each row represents one of the words in our vocabulary.
          So does each column. The matrix transition model
          treats a matrix as a lookup table. Find the row
          that corresponds to the word youâ€™re interested in. The value in
          each column shows the probability of that word coming next.
          Because the value of each element in the matrix represents
          a probability, they will all fall between zero and one.
          Because probabilities always sum to one, the values in each
          row will always add up to one.
        </p>
          <img title="Transition matrix"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/transition_matrix.png"
            alt="Transition matrix"
            style="height: 300px;">
        <p>
          In the transition matrix here we can see the structure
          of our three sentences clearly. Almost all of the transition
          probabilities are zero or one. There is only one place in
          the Markov chain where branching happens. After <em>my</em>,
          the words <em>directories</em>, <em>files</em>, or <em>photos</em>
          might appear, each with a different probability. Other than that,
          thereâ€™s no uncertainty about which word will come next.
          That certainty is reflected by having mostly ones and zeros in the
          transition matrix.
        </p>
        <p>
          We can revisit our trick of using matrix multiplication
          with a one-hot vector to pull out the transition probabilities
          associated with any given word. For instance, if we just wanted
          to isolate the probabilities of which word comes after <em>my</em>,
          we can create a one-hot vector representing the word <em>my</em>
          and multiply it by our transition matrix. This pulls out
          the relevant row and shows us the probability distribution
          of what the next word will be.
        </p>
          <img title="Transition probability lookup"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/transition_lookups.png"
            alt="Transition probability lookup"
            style="height: 300px;">
      ]]></description>
  </item>


  <item>
    <title>
    Transformers from Scratch 3/ðŸ§µ
    </title>
    <link>
    https://www.brandonrohrer.com/transformers
    </link>
    <pubDate>
    Fri, 15 Aug 2025 07:01:00 EDT
    </pubDate>
    <guid isPermaLink="false">
    2025081500
    </guid>
    <description><![CDATA[
        <h3 id="matrix_multiplication">Matrix multiplication</h3>
        <p>
          The dot product is the building block of matrix multiplication,
          a very particular way to combine a pair of two-dimensional arrays.
          We'll call the first of these matrices <em>A</em> and the second
          one <em>B</em>.
          In the simplest case, when <em>A</em> has only one row and
          <em>B</em> has only one column, the result of matrix multiplication
          is the dot product of the two.
        </p>
        <p style="text-align:center;">
          <img title="multiplication of a single row matrix and a single column matrix"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/matrix_mult_one_row_one_col.png"
            alt="multiplication of a single row matrix and a single column matrix"
            style="height: 300px;">
        </p>
        <p>
          Notice how the number of columns in <em>A</em> and the number of
          rows in <em>B</em> needs to be the same for the two arrays
          to match up and for the dot product to work out.
        </p>
        <p>
          When <em>A</em> and <em>B</em> start to grow, matrix multiplication
          starts to get trippy. To handle more than one row in <em>A</em>,
          take the dot product of <em>B</em> with each row separately.
          The answer will have as many rows as <em>A</em> does.
        </p>
        <p style="text-align:center;">
          <img title="multiplication of a two row matrix and a single column matrix"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/matrix_mult_two_row_one_col.png"
            alt="multiplication of a two row matrix and a single column matrix"
            style="height: 250px;">
        </p>
        <p>
          When <em>B</em> takes on more columns, take the dot product of
          each column with <em>A</em> and stack the results in successive
          columns.
        </p>
        <p style="text-align:center;">
          <img title="multiplication of a one row matrix and a two column matrix"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/matrix_mult_one_row_two_col.png"
            alt="multiplication of a one row matrix and a two column matrix"
            style="height: 250px;">
        </p>
        <p>
         Now we can extend this to mutliplying any two matrices, as long as
         the number of columns in <em>A</em> is the same as the number of
         rows in <em>B</em>. The result will have the same
         number of rows as <em>A</em> and the same number of columns as
         <em>B</em>.
        </p>
        <p style="text-align:center;">
          <img title="multiplication of a three row matrix and a two column matrix"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/matrix_mult_three_row_two_col.png"
            alt="multiplication of a one three matrix and a two column matrix"
            style="height: 450px;">
        </p>
        <p>
          If this is the first time you're seeing this, it might
          feel needlessly complex, but I promise it pays off later.
        </p>

        <h4 id="table_lookup">Matrix multiplication as a table lookup</h4>
        <p>
          Notice how matrix multiplication acts as a lookup table here.
          Our <em>A</em> matrix is made up of a stack of one-hot vectors.
          They have ones in the first column, the fourth column,
          and the third column, respectively. When we work through the
          matrix multiplication, this serves to pull out the first row,
          the fourth row, and the third row of the <em>B</em> matrix,
          in that order. This trick of using a one-hot vector to pull
          out a particular row of a matrix is at the core of how
          transformers work.
        </p>
    ]]></description>
  </item>

  <item>
    <title>
    Transformers from Scratch  2/ðŸ§µ
    </title>
    <link>
    https://www.brandonrohrer.com
    </link>
    <pubDate>
    Thu, 14 Aug 2025 20:01:00 EDT
    </pubDate>
    <guid isPermaLink="false">
    2025081405
    </guid>
    <description><![CDATA[
          <h3 id="dot_product">Dot product</h3>
        <p>
          One really useful thing about the one-hot representation is that
          it lets us compute
          <a href="https://en.wikipedia.org/wiki/Dot_product">dot products</a>.
          These are also known by other
          intimidating names like inner product and scalar product.
          To get the dot product of two vectors, multiply their
          corresponding elements, then add the results.
        </p>
        <p style="text-align:center;">
          <img title="Dot product illustration"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/dot_product.png"
            alt="Dot product illustration"
            style="height: 300px;">
        </p>
        <p>
          Dot products are especially useful when we're working with our
          one-hot word representations. The dot product of any one-hot
          vector with itself is one.
        </p>
        <p style="text-align:center;">
          <img title="Dot product of matching vectors"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/match.png"
            alt="Dot product of matching vectors"
            style="height: 300px;">
        </p>
        <p>
          And the dot product of any one-hot vector with any other one-hot
          vector is zero.
        </p>
        <p style="text-align:center;">
          <img title="Dot product of non-matching vectors"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/non_match.png"
            alt="Dot product of non-matching vectors"
            style="height: 300px;">
        </p>
        <p>
          The previous two examples show how dot products
          can be used to measure similarity. As another example,
          consider a vector of values that represents a combination of words
          with varying weights.
          A one-hot encoded word can be compared against it with the
          dot product
          to show how strongly that word is represented.
        </p>
        <p style="text-align:center;">
          <img title="Dot product gives the similarity between two vectors"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/similarity.png"
            alt="Dot product gives the similarity between two vectors"
            style="height: 300px;">
        </p>
    ]]></description>
  </item>

  <item>
    <title>
    Transformers from Scratch  1/ðŸ§µ
    </title>
    <link>
    https://www.brandonrohrer.com
    </link>
    <pubDate>
    Wed, 13 Aug 2025 22:01:00 EDT
    </pubDate>
    <guid isPermaLink="false">
    2025081400
    </guid>
    <description><![CDATA[
<p>
Behind all the GPTs and Geminis and Claudes and Llamas are transformers. They were introduced as a tool for sequence transductionâ€”converting one sequence of symbols to anotherâ€”and are now used almost entirely for sequence completionâ€”given a starting prompt, carry on in the same vein and style.
</p>
<p>
Here is a deep dive into the nuts and bolts of what make them tick. This isn't a short journey, but I hope you'll be glad you came.
</p>

<h2>One-hot encoding</h2>
<p>
In the beginning were the words. So very many words. Our first step is to convert all the words to numbers so we can do math on them.
</p>
<p>
Imagine that our goal is to create the computer that responds to our voice commands. Itâ€™s our job to build the transformer that converts (or transduces) a sequence of sounds to a sequence of words.
</p>
<p>
We start by choosing our vocabulary, the collection of symbols that we are going to be working with in each sequence. In our case, there will be two different sets of symbols, one for the input sequence to represent vocal sounds and one for the output sequence to represent words.
</p>
<p>
For now, let's assume we're working with English. There are tens of thousands of words in the English language, and perhaps another few thousand to cover computer-specific terminology. That would give us a vocabulary size that is the better part of a hundred thousand. One way to convert words to numbers is to start counting at one and assign each word its own number. Then a sequence of words can be represented as a list of numbers.
</p>
<p>
For example, consider a tiny language with a vocabulary size of three: files, find, and my. Each word could be swapped out for a number, perhaps files = 1, find = 2, and my = 3. Then the sentence "Find my files", consisting of the word sequence [ find, my, files ] could be represented instead as the sequence of numbers [2, 3, 1].
</p>
<p>
This is a perfectly valid way to convert symbols to numbers, but it turns out that there's another format that's even easier for computers to work with, one-hot encoding. In one-hot encoding a symbol is represented by an array of mostly zeros, the same length of the vocabulary, with only a single element having a value of one. Each element in the array corresponds to a separate symbol.
</p>
<p>

Another way to think about one-hot encoding is that each word still gets assigned its own number, but now that number is an index to an array. Here is our example above, in one-hot notation.
</p>
        <p style="text-align:center;">
          <img title="A one-hot encoded vocabulary"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/one_hot_vocabulary.png"
            alt="A one-hot encoded vocabulary"
            style="height: 300px;">
        </p>
        <p>
          So the sentence "Find my files" becomes a sequence of one-dimensional
          arrays,
          which, after you squeeze them together,
          starts to look like a two-dimensional array.
        </p>
        <p style="text-align:center;">
          <img title="A one-hot encoded sentence"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/one_hot_sentence.png"
            alt="A one-hot encoded sentence"
            style="height: 300px;">
        </p>

<p>
So the sentence "Find my files" becomes a sequence of one-dimensional arrays, which, after you squeeze them together, starts to look like a two-dimensional array.
</p>
<p>
Heads-up, I'll be using the terms "one-dimensional array" and "vector" interchangeably. Likewise with "two-dimensional array" and "matrix".
</p>
    ]]></description>
  </item>

</channel>
</rss> 
