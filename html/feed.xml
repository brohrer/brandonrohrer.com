<rss version="2.0">
<channel>
<title>Brandon Rohrer</title>
<link>https://www.brandonrohrer.com</link>
<description>Brandon Rohrer's blog</description>

  <item>
    <title>
    (Web)Traffic Control 
    </title>
    <link>
    https://brandonrohrer.com/hosting5.html
    </link>
    <pubDate>
    Wed, 08 Oct 2025 08:36:00 EDT
    </pubDate>
    <guid>
    https://brandonrohrer.com/hosting5.html
    </guid>
    <description><![CDATA[


<p>
The internet is a pretty rough and tumble place to hang out.
It's not even that there are so many jerks out there, it's just that
all of them can reach you at once.
Luckily you have the power to shut them down.
</p>

<p>
This installment focuses on blocking IP addresses that are doing things
they shouldn't. It's the lastest in a series of posts on
</p>

<ol>
<li> <a href="hosting.html">Setting up a webserver</a></li>
<li> <a href="hosting2.html">Setting up a domain name</a></li>
<li> <a href="hosting3.html">Setting up some security</a></li>
<li> <a href="hosting4.html">Keeping a webserver healthy</a></li>
</ol>

<h2><a id="Block-an-IP-address"></a><a href="#Block-an-IP-address">Block an IP address</a></h2>

<p>
The most straightforward way to block and IP address is in the firewall.
It is the tool build specifically for this.
</p>

<p>
To block the address <code>101.101.101.101</code>, run from the command line
</p>

<p>
<pre>
sudo ufw insert 1 deny from 101.101.101.101
</pre>
</p>

<p>
This instructs ufw (the Uncomplicated FireWall) to insert a rule at the
the top of the list (position 1) to deny all incoming traffic from
the address. After running this, no restart of the firewall is needed.
The rule is active.
(<a href="https://www.digitalocean.com/community/tutorials/ufw-essentials-common-firewall-rules-and-commands)">ufw docs</a>
</p>

<p>
The position 1 is important because in ufw, the first rule that matches
is applied. If there was a rule to allow all addresses that started
with <code>101.</code> and that rule came before the deny rule, then the deny
rule would never be reached.
</p>

<p>
While it's possible to block specific ports, or even to block an
IP address from seeing particular pages, complex rules
and conditions get difficult to analyze very quickly, and can lead to cases
where there are loopholes. Use fancy rule combinations sparingly.
</p>

<h2><a id="Parse-logs"></a><a href="#Parse-logs">Parse logs</a></h2>

<p>
In their raw form access logs are technically human-readable, but they are
a lot. I found it really useful to do a little parsing to pull out the bits
I'm interested in.
(I'm working with the default nginx log format, so adjust this
according to your own.)
</p>

<p>
I <a href="https://codeberg.org/brohrer/webserver-toolbox/src/branch/main/log_reader.py">wrote a script</a>
to take advantage of the repeatable structure of these logs
to dissect them into their parts. It uses tricks like splitting the log
based on brackets and spaces. It productes a pandas dataframe with
columns containing the IP address, requested URI, HTTP status code,
and every component of the date and time.
</p>

<p>
If you use exactly the same setup and log format as I do you might be
able to get away with using this code right out of the box. More likely
you'll have to (or want to) adjust it a bit for your own circumstances.
Make it your own!
</p>

<p>
This better organized version of the log can be used to generate a list
of pages that were queried, a list of IP addresses that visited the site,
or even just a chronological list of every access attempt.
</p>

<p>
Here's a
<a href="https://codeberg.org/brohrer/webserver-toolbox/src/branch/main/pages.py">pages.py</a>
script that shows how many times pages were hit in a given day, for example
</p>

<p>
<figure>
  <img title="Count and page names" alt="Sample output from pages.py
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/pages_outputs.png">
  <figcaption>Count and page names</figcaption>
</figure>
</p>

<p>
Here's a
<a href="https://codeberg.org/brohrer/webserver-toolbox/src/branch/main/ips.py "Count and IP address"">ips.py</a>
script that shows how many times a particular IP address visited that day
</p>

<p>
<img alt="Sample output from ips.py
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/ips_outputs.png">
</p>

<p>
Here's a
<a href="https://codeberg.org/brohrer/webserver-toolbox/src/branch/main/history.py">history.py</a>
script that just repeats the access logs directly, but in a stripped down format that's easier to read.
</p>

<p>
<figure>
  <img title="Time, HTTP status, IP address, and page name" alt="Sample output from history.py
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/history_outputs.png">
  <figcaption>Time, HTTP status, IP address, and page name</figcaption>
</figure>
</p>

<h2><a id="Browsing-the-logs"></a><a href="#Browsing-the-logs">Browsing the logs</a></h2>

<p>
At first glance these logs are just an unbroken wall of text, but after
spending a couple of minutes with them, oddities emerge.
</p>

<p>
Looking at the IP address access count, why is there one address that accessed
the website 633 times? That's more than four times the next most frequent.
What was happening there? Surely that can't be legit, can it?
</p>

<p>
Looking at the access history, why was someone trying to find a
2016 New York Times article on this website? (The funny characters that come
before are a utf-8 encoding of a unicode right double quotation mark.)
That looks like someone was really flailing.
</p>

<p>
Looking at the pages viewed, once you get past the stylesheets,
javascripts, feed.xml., and robots.txt, there is the top-level blog,
a popular post about transformers which I put a ton of effort into, and
then <code>ssh_at_home</code>, a hastily-written set of notes about setting
up an ssh server for personal use. Why is that one so regularly visited?
</p>

<p>
The longer you look, the more questions arise. Some of the most
interesting patterns come from people doing mischief.
</p>

<h2><a id="Bad-behavior-#1:-Scanning-for-secrets"></a><a href="#Bad-behavior-#1:-Scanning-for-secrets">Bad behavior #1: Scanning for secrets</a></h2>

<p>
<figure>
  <img title="Looking for juicy passwords and tokens" alt="Access log history showing scanning for secrets
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/scanning_secrets.png">
  <figcaption>Looking for juicy passwords and tokens</figcaption>
</figure>
</p>

<p>
Here, a single IP address quickly tries to access a handful of variations
on a <code>.env</code> file, which is a common place to store access tokens
and other credentials. There is no good reason to be doing this, unless
you are doing it to yourself, proactively scanning for vulnerabilities
in order to fix them.
</p>

<p>
In my judgment this is a one-strike-and-you're out behavior. This IP
address will get added to my block list.
Luckily we already blocked access to all dot-files in the nginx server block
during initial setup. That's why these are resulting in a 403 status code
(access denied) rather than a 404 (not found). But if someone is doing this
they may be looking for other ways into your system. Why not just block
them at the firewall?
</p>

<h2><a id="Bad-behavior-#2:-Fishing-for-files"></a><a href="#Bad-behavior-#2:-Fishing-for-files">Bad behavior #2: Fishing for files</a></h2>

<p>
<figure>
  <img title="Someone really wants my website to be written in php" alt="Access log history showing fishing for php files
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/fishing_files.png">
  <figcaption>Someone really wants my website to be written in php</figcaption>
</figure>
</p>

<p>
In this segment, a single IP address is trying to find a
php file, and they appear to be randomly casting about. These files
don't exist on my webserver and never have. This requestor appears to
be shooting in the dark for files that they do not have a link to.
I don't know why, but I don't like it. I have all
php files blocked in my server block, but still this behavior shows someone
doing something other than browsing my website, which leads me not
to trust them. For me, this is a blockable offense.
</p>

<p>
Other file fishing I see often is for WordPress-related files.
</p>

<p>
<figure>
  <img title="Someone really wants my website to be written in WordPress" alt="Access log history showing fishing for WordPress files
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/fishing_wordpress.png">
  <figcaption>Someone really wants my website to be written in WordPress</figcaption>
</figure>
</p>

<p>
I also get requests trying to access links from my pages, as if they were
hosted on my server, such as
<code>/%20%20%20%20%20%20%20%20%20%20%20%20https:/en.wikipedia.org/wiki/Convolution</code>.
The <code>%20</code> is URL encoding for a space character. I also don't know why
there are 12 spaces in front of the URL. It looks like sloppy automated
parsing of a bot accessing links that were extracted from my html files.
I don't know what benign purpose this could serve. I'm content to
block these as well.
</p>

<p>
With the history browser there is a <code>--status</code> argument that lets you
quickly see just the 404's and 403's and 400's (bad request error)
and helps the file fishing to pop out.
</p>

<p>
<pre>
uv run history.py --status 404
</pre>
</p>

<h2><a id="Bad-behavior-#3:-Rapid-fire-requests"></a><a href="#Bad-behavior-#3:-Rapid-fire-requests">Bad behavior #3: Rapid-fire requests</a></h2>

<p>
<figure>
  <img title="More than ten requests per second is a lot." alt="Access log history showing two dozen requests in the span of two seconds
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/too_fast_requests.png">
  <figcaption>More than ten requests per second is a lot.</figcaption>
</figure>
</p>

<p>
Sometimes an IP address will make a lot of requests in quick succession.
Most of the time this is a mild annoyance, but when taken to an extreme
it floods the server, preventing any other requests from getting through.
It denies everyone else service, earning the name Denial of Service (DOS),
and whether done maliciously or through negligence, the effect on your
website is the same.
</p>

<p>
A lot of back-to-back requests is almost always a hallmark of automated
scraping. It's a personal judgment call, how much you want to support this.
The intended audience for my website is individuals, rather than
AI-training companies or even search engines, so I'm comfortable
making life uncomfortable for bulk-scrapers. There are two ways to do this.
</p>

<p>
The first is to set up rate limiting. One rate limiting mechanism is a
polite request in the <code>robots.txt</code> file to limit requests to, say,
one every 5 seconds.
</p>

<p>
<pre>
User-agent: *
Crawl-delay: 5
</pre>
</p>

<p>
Unfortunately, manners are in short supply on the internets, and most
crawlers and scrapers, including Google, ignores this directive. We can
resort to more draconian measures and use nginx to implement per-IP rate
limiting on the webserver.
</p>

<p>
This is done with a modification to the server block, as
<a href="https://codeberg.org/brohrer/webserver-toolbox/src/commit/ad139aa48eb2d173d9dc9114759e48c485375b9e/server_blocks/brandonrohrer.com#L4">here</a>
and
<a href="https://codeberg.org/brohrer/webserver-toolbox/src/commit/ad139aa48eb2d173d9dc9114759e48c485375b9e/server_blocks/brandonrohrer.com#L15">here</a>
</p>

<p>
<pre>limit_req_zone $binary_remote_addr zone=one:1m rate=1r/s;<br>
server {
    limit_req zone=one burst=10 nodelay;
    limit_req_status 429;
}
</pre>
</p>

<p>
These lines create a "zone", a 1MB history of IP addresses
that have made requests. On average, they should be making no more than
1 request per second. It allows for "bursts" of 10 additional requests,
serving them immediately with no delay, but anything in excess of that which
violates the rate limit will receive a HTTP status code of 429
(too many requests).
</p>

<p>
There are a lot of possible variations to this, but this is the basic
pattern. For a deep dive, check out
<a href="https://nginx.org/en/docs/http/ngx_http_limit_req_module.html">the nginx docs</a>.
</p>

<p>
And of course it's always possible to block IP addresses who try to pull this.
</p>

<h2><a id="Bad-behavior-#4:-Trying-to-hide-rapid-fire-requests"></a><a href="#Bad-behavior-#4:-Trying-to-hide-rapid-fire-requests">Bad behavior #4: Trying to hide rapid-fire requests</a></h2>

<p>
<figure>
  <img title="You're not fooling anyone." alt="Access log history showing three dozen coordinated requests in one second
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/botnet.png">
  <figcaption>You're not fooling anyone.</figcaption>
</figure>
</p>

<p>
Given how easy it was to rate limit a single IP address it should come as no
surprise that enterprising webscrapers have found a cheat. If they split their
requests across a whole bunch of IP addresses, then rate limiting on a
single address doesn't slow them down.
</p>

<p>
This even hides behing a respectable-sounding techy name: <a href="https://medium.com/@datajournal/best-rotating-proxies-tested-303539da1e2a">rotating proxies</a>.
But underneath it's just a way to get around website admins' express desire
that these jerks <em>not</em> do this thing.
</p>

<p>
Detecting this is trickier. In the example above, it's clear to a human
eye, that the requests are part of a coordinated scraping operation.
They all occur within one second. They are all requesting a .png
within the same directory. The IP addresses, while containing just
a few repeats, fall into a handful of clusters. But writing rules
for automating this is hard. Every rule you can come up with will
probably miss some coordinated scraping, or deny some legitimate traffic,
or both. Even machine learning methods, which can take multiple factors
into account, may not be able to do this cleanly.
</p>

<p>
Of course dangling a tricky problem like this in front of nerd is like waving
a red cape in front of an angry bull. I'll probably come back to deeper
treatment of it later. For now the only strategy I can recommend is manually
blocking every single IP address involved.
</p>

<h2><a id="Bad-behavior-#5:-Attempting-unsupported-actions"></a><a href="#Bad-behavior-#5:-Attempting-unsupported-actions">Bad behavior #5: Attempting unsupported actions</a></h2>

<p>
<figure>
  <img title="Someone with PROPFIND is not to be deterred." alt="Some HTTP request actions other than GET
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/illegal_actions.png">
  <figcaption>Someone with PROPFIND is not to be deterred.</figcaption>
</figure>
</p>

<p>
There are just a handful of things you can do over HTTP. <code>GET</code> and <code>POST</code> are
the most common, and <code>HEAD</code> comes up sometimes (get info about a page
without downloading it) but there are others that almost never come up in
the normal course of events. <code>PROPFIND</code> is a way to gather information about
all the available files on a server. It's easy to imagine how convenient that
would be for a scraper. <code>CONNECT</code> sets up an open pipe for data to flow to
and from a server. Nothing I would want to enable for a client I don't know
and trust.
</p>

<p>
For my little static website in particular, the only valid actions are
<code>GET</code> and <code>HEAD</code>. There is nothing to <code>PUT</code> or <code>POST</code> and everything
else I have no intention of allowing. I've never offered these actions
for any purpose, and it's very likely that any requests that contain them
are trying to get access to data and functionality they shouldn't have.
Block-worthy behavior in my book.
</p>

<h2><a id="Blocking-revisited"></a><a href="#Blocking-revisited">Blocking revisited</a></h2>

<p>
From the list above, there are a lot of offenses that can get an IP
address blocked, and a lot of IP addresses that commit them.
It would be possible to manually update the firewall rules for each one
at the command line by running something like this
</p>

<p>
<pre>
sudo ufw insert 1 deny from 101.101.101.101
</pre>
</p>

<p>
for every blocked address, but after my blocklist passed several dozen addresses
it started to feel tedious.
Updating the list became difficult after the list passed 100 addresses.
When adding new addresses manually I didn't want to duplicate the ones
that were already there, so I sorted them and added new ones into the
list where they belong. Duplicates were readily apparent. But after the
list grew to fill several screens lengths, updating became slow.
</p>

<p>
To streamline, I created a
<a href="https://codeberg.org/brohrer/webserver-toolbox/src/branch/main/blocklist_additions.txt">blocklist_additions.txt</a>
with every IP address I wanted to disallow. Then a small Python script
<a href="https://codeberg.org/brohrer/webserver-toolbox/src/branch/main/update_firewall.py">update_firewall.py</a>
was all I needed to automatically run the list.
</p>

<p>
<pre>
sudo python3 update_firewall.py
</pre>
</p>

<p>
This lets me browse through the logs and manually add problematic
IP addresses to the file <code>blocklist_additions.txt</code> on the server.
Then I can update the firewall with the latest changes.
</p>

<p>
Some of these behavior violations should be straightforward to detect
programmatically. Automating firewall updates will be an adventure
for another day.
</p>

<p>
October 6, 2025
</p>
    ]]></description>
  </item>

  <item>
    <title>
    Caring for your webserver
    </title>
    <link>
    https://brandonrohrer.com/hosting4.html
    </link>
    <pubDate>
    Thu, 25 Sep 2025 08:34:00 EDT
    </pubDate>
    <guid>
    https://brandonrohrer.com/hosting4.html
    </guid>
    <description><![CDATA[


<p>
In parts one through three, we
<a href="hosting.html">set up a web server</a>,
<a href="hosting2.html">connected it to a domain name</a>, and
<a href="hosting3.html">instituted some basic security</a>.
Now comes the fun part! The web server is humming along doing its thing,
and we can watch and admire, making little improvements here and there.
Itâ€™s not required, but for some, this is part of the payoff.
</p>

<h2><a id="A-webserver-needs-weeding-and-watering"></a><a href="#A-webserver-needs-weeding-and-watering">A webserver needs weeding and watering</a></h2>

<p>
It can be counterintuitive that a thing made out of code should need
ongoing attention and care. On the surface it seems like it should be
self-sufficient, like a wall made of stones.
We put the pieces in place where we want them and when weâ€™re happy with it, we stop.
It's all silicon and bits after all, why should it need watching?
</p>

<p>
The bigger picture, though, is that the web server operates in a world
thatâ€™s always changing. Software updates cause tools to behave differently.
Edits happen to the HTML and other content we host.
There are dramatic changes in who is trying to reach the content
and for what purposes. There can be outages, policy changes, and any
number of second-order effects in the wider world that can make our
web server stop operating the way we want. So on that scale a web server
starts to more closely resemble a vegetable garden&mdash;something growing,
decaying, and very much a product of the environment that it's in.
</p>

<p>
This page is my notes on care and feeding practices I've found helpful
and enjoyable. I'm not an expert on this, and this is not authoritative
by any means. But I put it here in case you find it helpful. If I missed
anything, or got it egregiously wrong, please let me know.
</p>

<h2><a id="Setting"></a><a href="#Setting">Setting</a></h2>

<p>
In these examples I'm working with a
<a href="https://cloud.digitalocean.com">DigitalOcean droplet</a> at <code>138.197.69.146</code>
hosting the content for the domain <code>brandonrohrer.com</code>. It's running
an nginx server on Ubuntu 24.04.
My local machine is a MacBook pro, where I'm working
from the Terminal. My go-to text editor is vim, but you can use nano
instead.  Adjust the snippets below for your situation.
</p>

<h2><a id="Browse-the-logs"></a><a href="#Browse-the-logs">Browse the logs</a></h2>

<p>
I've been curious about self-hosting for a while, but the thing that pushed
me over the edge into migrating off Netlify was a desire to
see which IP addresses were visiting which pages when. This information
is all in the logs.
</p>

<p>
Because I set up a server block specifically for my domain,
the log files live in <code>/var/log/nginx/brandonrohrer.com/</code>.
</p>

<p>
<code>access.log</code> has the main logs, and <code>error.log</code> keeps a log of when
things go poorly.
</p>

<p>
To browse the logs
<pre>
cat /var/log/nginx/brandonrohrer.com/access.log
</pre>
</p>

<p>
which gives something like
</p>

<p>
<pre>
49.51.195.195 - - [14/Sep/2025:08:20:50 -0400] "GET /hosting2 HTTP/1.1" 200 5957 "-" "Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1"
183.98.90.239 - - [14/Sep/2025:08:20:50 -0400] "GET /images/ml_logo.png HTTP/1.1" 301 178 "https://www.brandonrohrer.com/blog.html" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36"
24.126.100.175 - - [14/Sep/2025:08:21:02 -0400] "GET /feed.xml HTTP/1.1" 304 0 "-" "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:136.0) Gecko/20100101 Firefox/136.0"
52.187.246.128 - - [14/Sep/2025:08:21:16 -0400] "GET /transformers.html HTTP/1.1" 200 36194 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko); compatible; ChatGPT-User/1.0; +https://openai.com/bot"
</pre>
</p>

<p>
or
</p>

<p>
<pre>
cat /var/log/nginx/brandonrohrer.com/error.log
</pre>
</p>

<p>
which gives errors like
</p>

<p>
<pre>
2025/09/14 00:37:00 [error] 187956#187956: *28044 access forbidden by rule, client: 78.153.140.50, server: brandonrohrer.com, request: "GET /.env.bak HTTP/1.1", host: "138.197.69.146"
2025/09/14 00:37:00 [error] 187956#187956: *28045 access forbidden by rule, client: 78.153.140.50, server: brandonrohrer.com, request: "GET /mail/.env.db HTTP/1.1", host: "138.197.69.146"
2025/09/14 00:37:01 [error] 187956#187956: *28046 access forbidden by rule, client: 78.153.140.50, server: brandonrohrer.com, request: "GET /dev/.env.old HTTP/1.1", host: "138.197.69.146"
2025/09/14 00:37:01 [error] 187956#187956: *28047 access forbidden by rule, client: 78.153.140.50, server: brandonrohrer.com, request: "GET /crm/.env.bak HTTP/1.1", host: "138.197.69.146"
</pre>
</p>

<p>
Scanning through these for a few minutes can reveal some fascinating patterns,
a few of which will pop up later in the post.
</p>

<h2><a id="Check-for-missed-pages"></a><a href="#Check-for-missed-pages">Check for missed pages</a></h2>

<p>
With a slight tweak, this command can pull out only the logs containing
a "404", the http status code for Page Not Found. (It actually pulls out every
log that has a 404 anywhere in it, but the majority of these are
page not found entries.)
</p>

<p>
<pre>
cat /var/log/nginx/brandonrohrer.com/access.log | grep 404
</pre>
</p>

<p>
This slice of the logs reveals files I don't have, but are normal to
look for, like <code>/favicon.ico</code> and  <code>/.well-known/traffic-advice</code>. It
also showed mistakes like a misspelled filename or a missing image.
</p>

<h2><a id="Catch-pages-without-the-<code>.html</code>"></a><a href="#Catch-pages-without-the-<code>.html</code>">Catch pages without the <code>.html</code></a></h2>

<p>
One of the first surprises I got was looking 404's in the logs was
that visitors coming to my page looking for a page called
<code>transformers.html</code> would be turned away if they only put in <code>transformers</code>.
There were a lot of 404 log entries for misses of this sort. This was
a bummer. These were people who are trying to visit my website but
are being denied on a technicality.
</p>

<p>
Luckily nginx makes it straightforward to fix this.
I edited the file containing the server block
</p>

<p>
<pre>
sudo vi /etc/nginx/sites-available/brandonrohrer.com
</pre>
</p>

<p>
and changed the line that read
</p>

<p>
<pre>
try_files $uri $uri/ =404;
</pre>
</p>

<p>
so that it read
</p>

<p>
<pre>
try_files $uri $uri.html $uri/ =404;
</pre>
</p>

<p>
When nginx parses the webpage being requested, <code>$uri</code> is the variable
containing the page name. The modified line instructs the server to
first try the exact page name requested, then to try it with a <code>.html</code>
tagged onto the end, then to try it with a backslash on the end, and if
none of those turn anything up, return a 404 page not found error.
</p>

<p>
After making this change (or any of the other changes described below)
it's important to first test that we didn't goober anything up
</p>

<p>
<pre>
sudo nginx -t
</pre>
</p>

<p>
and if our change passes the test and nginx is happy with it, then restart
nginx so that the change takes effect
</p>

<p>
<pre>
sudo systemctl restart nginx
</pre>
</p>

<h2><a id="Redirects"></a><a href="#Redirects">Redirects</a></h2>

<p>
In addition to accounting for missing html extensions, I've found it helpful
to automatically expand short names into longer ones.
For example, <code>numba</code> automatically redirects to <code>numba_tips.html</code>.
It's also good for fixing typos in published pages. I've
redirected <code>statistics_resources.html</code> to <code>stats_resources.html</code> because
I shared the wrong URL in a publication.
</p>

<p>
I also use it as a link shortener, so that <code>fnc</code> actually points to a pdf
in a codeberg repository and <code>bp</code> redirects to a video about
backpropagation on youtube. For the record, <code>brandonrohrer.com</code> is a
terrible domain name for a link shortener. I also picked up <code>tyr.fyi</code>
as a shorter domain for when I want erally short links.
</p>

<p>
https://www.digitalocean.com/community/tutorials/nginx-rewrite-url-rules
</p>

<p>
Redirects are implemented in the domain's server block.
</p>

<p>
<pre>
sudo vi /etc/nginx/sites-available/brandonrohrer.com
</pre>
</p>

<p>
Add a line like this within the server block.
</p>

<p>
<pre>
location = /numba {
    return 301 $scheme://brandonrohrer.com/numba_tips.html;
}
</pre>
</p>

<p>
This instruction tells the server to look for a request that looks like
<code>brandonrohrer.com/numba</code> and forwards the request to
<code>brandonrohrer.com/numba_tips.html</code>. <code>$scheme</code> preserves the <code>http</code> or
<code>https</code>, whatever was used in the original request.
</p>

<p>
Similarly, these lines take any request  like <code>brandonrohrer.com/bp</code>
and redirect it toward a specific video URL.
</p>

<p>
<pre>
location = /bp {
    return 301 https://www.youtube.com/watch?v=6BMwisTZFr4;
}
</pre>
</p>

<h3><a id="Location-blocks"></a><a href="#Location-blocks">Location blocks</a></h3>

<p>
Redirects are an example of what can be done with location blocks.
They can also be used to rewrite requests, or block access.
</p>

<p>
In addition to exact matching on page names, location blocks can match
on partial names, directories, and regex-specified patterns.
They give fine grained control for how individual pages are accessed
and even which IP addresses are allowed to access them, but they
can quickly become complicated. Individual requests can match multiple
location blocks, and the match is not determined on a first-come,
first-served bases, but rather based on a set of rules. Use with due caution.
The DigitalOcean <a href="https://www.digitalocean.com/community/tutorials/understanding-nginx-server-and-location-block-selection-algorithms">docs on how location blocks get matched</a>
are a great resource if you want to dig into this.
</p>

<h2><a id="Set-up-log-rotation"></a><a href="#Set-up-log-rotation">Set up log rotation</a></h2>

<p>
By default, access logs are stored in <code>access.log</code> and error logs are
stored in <code>error.log</code> and those files just keep getting longer and longer.
I find it really helpful to have one an automatic log rotation in place
where each file contains just one day's access logs or errors. Todays
are called <code>access.log</code> and <code>error.log</code>, yesterday's are
<code>access.log.1</code> and <code>error.log.1</code>, and the day before that
<code>access.log.2</code>, et cetera, covering the last couple of weeks.
(Starting at day 2 they are also gzipped.) There is
<a href="https://www.digitalocean.com/community/tutorials/how-to-configure-logging-and-log-rotation-in-nginx-on-an-ubuntu-vps#managing-a-log-rotation">a great guide</a>
to setting this up in the DO docs. The meat of the setup involves
modifying the file <code>/etc/logrotate.d/nginx</code>
</p>

<p>
After modifying it, this is what my log rotation config looks like
</p>

<p>
<pre>
/var/log/nginx/brandonrohrer.com/*.log {
	daily
	missingok
	rotate 14
	compress
	delaycompress
	notifempty
	create 0644 www-data adm
	sharedscripts
	prerotate
		if [ -d /etc/logrotate.d/httpd-prerotate ]; then \
			run-parts /etc/logrotate.d/httpd-prerotate; \
		fi \
	endscript
	postrotate
		invoke-rc.d nginx rotate >/dev/null 2>&1
	endscript
}
</pre>
</p>

<h2><a id="Find-a-content-provider-for-larger-files"></a><a href="#Find-a-content-provider-for-larger-files">Find a content provider for larger files</a></h2>

<p>
If you start offering files larger than a couple megabytes and you start
getting more than a handful of views per day, your bandwidth requirements
can climb quickly. One way around this is to keep your large files like
video, audio, and big images, and other beefy files somewhere other than your
web server.
</p>

<p>
There are several services that offer image, audio, and video hosting, and
some of them have free tiers. YouTube is a popular option for hosting vides,
but I find ads and irrelevant recommendations so annoying that I've paid
a few dollars for a bottom-tier Vimeo account. I don't do a lot of
audio-only content, so no recommendations there. My biggest bandwidth
hog is my image catalog. I explored using a content hosting service for them
but wasn't interested in all the bells and whistles. I eventually settled
on abusing GitHub as a hosting service. There seem to be no limitations
on repository size, no throttling on bandwidth (at least at the scales
I'm using it for) and I'm OK with the trade-off of free hosting in
exchange for giving Microsoft unfettered access to my images.
</p>

<p>
As a result the bandwidth from my server tends to settle around 10 kB/s
on average, while serving a few thousand page views per day. It helps me
keep my hosting costs low.
</p>

<p>
<img alt="The baseline bandwidth of the webserver has an average of about 10 kb/s
" src="baseline_bandwith.png">
</p>
    ]]></description>
  </item>


  <item>
    <title>
    What can you do when the instituion fails you?
    </title>
    <link>
    https://www.brandonrohrer.com/org_response.html
    </link>
    <pubDate>
    Sun, 21 Sep 2025 08:36:00 EDT
    </pubDate>
    <guid>
    https://www.brandonrohrer.com/org_response.html#top
    </guid>
    <description><![CDATA[

 <h3>A leader in my organization did something I hate.</h3>
        <h3>What do I do now?</h3>

        <p>
          Poor behavior by an organizationâ€™s leaders isnâ€™t an uncommon
          scenario, nor is it a new one. Most of us will find ourselves in the
          situation at some point. What do we do when our academic
          institution/company/nonprofit does something unethical, illegal, or
          in really poor taste? How do we trade off the
          benefits of membership with the taint of association? This is an
          intensely personal question, and thereâ€™s no way I can answer it for
          you, but I can offer some starting points for considering your
          response.
        </p>

        <h3>It hurts</h3>

        <p>
          When this happens, the initial experience is one of pain. You may
          feel one or more of these:
        </p>

        <p>
          <strong>Betrayal of trust</strong>. This type of behavior is not what you signed on for. Maybe you just wanted satisfying employment or to make the world a little better place. And maybe leaders have said lofty things about values and vision. Then this happened. Itâ€™s a bait-and-switch and you feel betrayed.
        </p>

        <p>
          <strong>Personal injury</strong>. Leaders' actions may hurt you directly. Sexual harassment and discrimination are all-to-common ways for this to happen, but it can also come in the form of verbal abuse, recognition withheld, or failing to provide a physically safe work environment.
        </p>

        <p>
          <strong>Guilt</strong>. You may feel bad about yourself for being part of something that fails to live up to its promises. You might even feel personally accountable for leadersâ€™ actions. This can manifest as a persistent gnawing feeling in your stomach that makes it hard to get out of bed.
        </p>

        <p>
          <strong>Shame</strong>. Itâ€™s really natural to feel worried about what other people think&mdash;that they judge you as a sellout or a lemming or a pawn, weak or greedy or foolish. They may even say it in a tweet or to your face over holiday dinners. That stings.
        </p>

        <p>
          While you are feeling these, keep your eyes open for
          <a href=https://en.wikipedia.org/wiki/Gaslighting"">
            <strong>gaslighting</strong></a>, where someone
          tries to convince you that the problem is you.
          Either you
          misunderstood, or you are too sensitive, or there were some
          extenuating circumstances, or "everyone knows that's just how Steve
          is." Gaslighting is manipulative, and it plays on our need
          to calibrate our social interpretations with others. Stick to your
          guns. If something feels wrong or off to you, no one can gainsay
          that. If you start second-guessing your reaction, find a couple
          of people that you trust deeply and reality check your experiences
          with them. Your experiences are yours, and no one can tell you they
          aren't valid.
        </p>

        <h3>Consider your options</h3>

        <p>
          So it hurts. What can you do about it? You have a few options, and they each come with their own upside and downside.
        </p>

        <p>
          <strong>Gadfly</strong>. Criticize the bad behavior to other members
          of the organization. Make it clear that you disapprove.
        </p>

        <ul>
          <li>
            <em>Downside</em>: Leaders may not even be aware of your
            displeasure, much less feel pressure to change. This is a good
            coping mechanism, but may not resolve the issue. If leaders do
            become
            aware of your criticisms, they may move to smooth things over or
            they may try to keep you quiet.
          </li>
          <li>
            <em>Upside</em>: You are doing something. You donâ€™t have to suffer
            in silence. Also, you get a chance to commiserate with other
            members of your organization and support anyone who might be
            feeling isolated. And there is always a small chance that the
            offending leader will hear and see the error of their ways.
          </li>
        </ul>

        <p>
          <strong>Conscience</strong>. Call out leadersâ€™ poor decisions but emphasize opportunities for improvement. Make a call to be better. Provide specific suggestions for corrective actions.
        </p>

        <ul>

          <li>
            <em>Downside</em>: Change is hard and so the default human position
            is to avoid it whenever possible. If leaders are not deliberately
            looking to improve, they will probably do the minimum, which is to
            say a few of the right words without making any substantive
            changes. They also might dismiss the criticism or simply do
            nothing, compounding the original sin.
          </li>
          <li>
            <em>Upside</em>: This is a very constructive approach. It is an
            expression of trust in your leaders.
            Compared to the criticism of the Gadly, it can be easier to hear
            and respond to a Conscience while saving face.
            It gives well meaning and
            responsive leaders a chance to improve and heal the organization.
          </li>
        </ul>

        <p>
          <strong>Dissenter</strong>.
          While maintaining
          affiliation with the organization, condemn the objectionable
          behavior in a public forum, like Twitter or the New York Times.
          A Dissenter is like a Gadfly, except that they
          air their grievances publicly.
        </p>

        <ul>
          <li>
            <em>Downside</em>: Unfortunately, most organizations don't take
            well to this. If you are vocal enough, you will likely be
            marginalized within the organization or even kicked out. This can
            cost a lot, both personally and professionally.
          </li>
          <li>
            <em>Upside</em>: This makes a strong statement to all your social
            and professional circles that you condemn the behavior. It puts distance between you
            and your organization and provides an escape from
            guilt and shame. When conditions are right, this can produce a
            great deal of pressure on the organization to change.
          </li>
        </ul>

        <p>
          <strong>Saboteur</strong>. Stay part of the organization but deliberately
          damage it. Common approaches include obstructing operations or leaking confidential
          information.
        </p>

        <ul>

          <li>
            <em>Downside</em>: It is possible that whatever harm you do will
            touch people in the organization that were not responsible for the
            offenses. You'll need to consider your path carefully if you want
            to avoid collateral damage.
          </li>
          <li>
            There's also a risk that leaders will seek retribution.
            Depending on the circumstances, your reputation may be attacked,
            you may be sued, you may be ostracized. Very large organizations
            have a great deal of power. In extreme cases you can literally
            spend the rest of your life suffering the consequences. I don't say
            this to dissuade you, but to encourage you to think through your
            options carefully.
          </li>
          <li>
            <em>Upside</em>: You can do real damage this way. If you have lost faith in an organization's leaders completely and have been personally harmed, this method offers a bit of justice.
          </li>
        </ul>
        <p>
          <strong>Ghost</strong>. Cut affiliation with the organization.
          Find another job, another community. Move on.
        </p>

        <ul>
          <li>
            <em>Downside</em>: The original infraction goes
            unaddressed. There is no resolution, and no guarantee that the same
            thing won't happen again. You also lose whatever it was that drew
            you to the organization to start with - a salary, a community, a
            common cause.
          </li>
          <li>
            <em>Upside</em>: The immediate cause of your pain may be removed.
            You get the chance to extricate yourself and start over. This is
            can be helpful when you have little hope of redress or improving the
            organization. You cut your losses and move on.
          </li>
        </ul>

        <p>
          <strong>Apostate</strong>. Leave the organization and actively attack
          it from the outside.
        </p>

        <ul>
          <li>
            <em>Downside</em>: Like the Ghost, you give up whatever benefits of membership you
            had. And like the Saboteur, you may end up harming those still in
            the organization that bear no responsibility for leaders'
            actions.
          </li>
          <li>
            <em>Upside</em>: You are more free to operate, since you don't need
            to worry about complying with the organizations norms (or at least
            appearing to). Your criticism can flow freely. You are not
            untouchable, especially to very powerful organizations, but it
            raises the bar for them to retaliate. They can't hold threat of
            expulsion over you. Publicly dismissing you and
            legal recourse are more common responses.
          </li>
        </ul>

        <p>
          <strong>Do nothing</strong>.
        </p>

        <ul>
          <li>
            <em>Downside</em>: Nothing changes. Whatever bad thing happened
            might well happen again. And again.
          </li>
          <li>
            <em>Upside</em>: Sometimes it's not worth the fight. If you are
            close to retirement or vesting it can be easiest just to ride it
            out. There are lots of instances where
            the personal connections or financial benefits of membership
            outweigh whatever embarassment a leader has caused.
            Sometimes, you can't afford to just walk away.
          </li>
          <li>
            Sometimes there just aren't any other viable options. The
            uncertainty of leaving a familiar organization can bring terror and
            paralysis. The thought of leaving friends and colleagues behind
            without support in
            a toxic environment can be profoundly distasteful.
          </li>
          <li>
            If you stay in the organization, you keep whatever goodness is there. You get to continue
            supporting your colleagues and community and drawing support from
            them. There may still be great opportunities for doing good,
            strengthening a community, or financially supporting your loved ones.
          </li>
          <li>
            The Do Nothing strategy can also be accompanied by a mild strain
            of the Saboteur in which you withold your best efforts. After all,
            it's hard to be committed to an organization when you
            feel disappointed or ashamed of its leaders.
          </li>
        </ul>

        <p>
          Choosing what to do can be agonizing.
          There are some big questions to consider. How are the
          ogranization's leaders likely to respond?
          Whatâ€™s will be the benefit to me and those I care about?
          And what's the cost?
        </p>
        <p>
          Above all else, be true to yourself. You might feel strongly that you
          need to do something without being able to say exactly why. Watch for
          strong feelings that don't fade
          after you've slept on them a couple of times. Thatâ€™s a
          strong signal. Hang on to that deep-seated conviction, even if well-meaning
          people you trust try to talk you out of it. If you donâ€™t follow
          through with it, you are likely
          to regret it for a long time.
        </p>

        <h3>The leaders of <em>someone elseâ€™s</em> organization did something very
          wrong.</h3>
        <h3>What do I do?</h3>

        <p>
          This is also a way-too-common situation. Again, the decision of whether and how to respond is yours alone, but I can offer a few points to consider.
        </p>

        <ul style="list-style-type:disc">
          <li>
            <strong>Organizations are made of individual people</strong>. A company is made up of the
            individuals. A board is made up of individuals. A committee is made
            up of individuals. Individual leaders made very poor decisions.
            The majority of the people in that organization are not responsible
            for those decisions, and may not support them.
          </li>

          <li>
            On a related note, <strong>be gentle when judging members of the
            organization that stay</strong> or fail to condemn it. There's no way to
            know for sure what their constraints or motivations are. They may
            be doing the best they can manage at the moment.
          </li>

          <li>
            Don't put much weight on what a leader says when called out.
            Instead, watch what they do. A sincere
            and specific apology is a great start (and rare enough), but when
            accompanied by repeated offenses it doesn't count for much.
            <strong>Actions
            speak louder than words</strong>. I weight the importance actions-to-words about 20-to-1.
          </li>

          <li>
            Pointing at another organizationâ€™s failings donâ€™t make up for those
            in your own. <strong>Make sure you're not just distracting
            yourself</strong> from
            problems closer to home or trying to redirect shame away from
            yourself.
          </li>

          <li>
            Labeling organizations as "evil" tends not to be useful. Regardless
            of their stated purpose, nearly every organization has some instances of bad
            behavior. Social irresponsibility, ethical sketchiness, or personal
            disrepect can be found almost anywhere if you look closely enough.
            Sadly, it seems to be a constant in any reasonably-sized group of
            humans. By
            some yardsticks, every organization is evil, and the word loses meaning. It's
            more useful to look at the specifics of the situation. How much
            harm was done? How powerless were the recipients? Was the incident
            isolated or part of a long-running pattern? Was it quickly remedied
            or minimized and covered up? <strong>Keeping your analysis
            concrete</strong> will help you more clearly navigate the trade-offs as you
            consider your response.
          </li>
          <li>
            Consider your goals. An online rant feels good but quickly fades,
            like a rock thrown into a pond.
            <strong>Reporting specific events and advocating
            specific corrections</strong> carries far more weight and is
            longer-lived.
          </li>

        </ul>

        <p>
          I hope some of the ideas here are helpful to you. If you have additions
          to suggest or a story to tell, don't hesitate to send me an email
          (brohrer@gmail.com) or post it on Twitter (@_brohrer_).
        </p>

        <p>
          I struggled with when to release this post. It's natural to look at whatever happened in the news yesterday and assume it was a trigger. But this has been brewing for decades.
              Iâ€™m not an expert in this area. These are opinions only.
              Most importantly <strong>I haven't experienced what you've
              experienced</strong>. Take anything here that's useful and
              discard the rest.
        </p>

        <p>
          Know that the frustration, anger, and/or sadness that you feel is
          legitimate and that you have some options. I wish you well as you
          find your path.
        </p>

    ]]></description>
  </item>


  <item>
    <title>
    Transformers from Scratch 12/ðŸ§µ
    </title>
    <link>
    https://www.brandonrohrer.com/transformers.html#positional_encoding
    </link>
    <pubDate>
    Fri, 19 Sep 2025 08:34:00 EDT
    </pubDate>
    <guid>
    https://www.brandonrohrer.com/transformers.html#positional_encoding
    </guid>
    <description><![CDATA[

   <h3 id="positional_encoding">Positional encoding</h3>
        <p>
          Up to this point, we've assumed that the positions of words
          are ignored, at least for any words coming before the
          very most recent word. Now we get to fix that using
          positional embeddings.
        </p>
        <p>
          There are several ways that position information could be introduced
          into our embedded represetation of words, but the way it was
          done in the original transformer was to add a circular wiggle.
        </p>
        <p style="text-align:center;">
          <img title="Positional encoding introduces a circular wiggle"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/positional_encoding.png"
            alt="Positional encoding introduces a circular wiggle"
            style="height:300px;">
        </p>
        <p>
          The position of the word in the embedding space acts as the
          center of a circle. A perturbation is added to it, depending
          on where it falls in the order of the sequence of words.
          For each position, the word is moved the same distance but
          at a different angle, resulting in a circular pattern as you
          move through the sequence.
          Words that are close to each other in the sequence have similar
          perturbations, but words that are far apart are perturbed
          in different directions.
        </p>
        <p>
          Since a circle is a two dimensional figure, representing
          a circular wiggle requires modifying two dimensions of the
          embedding space.
          If the embedding space consists of more than two dimensions
          (which it almost always does), the circular wiggle is
          repeated in all the other pairs of dimensions, but with different
          angular frequency, that is, it sweeps out a different number of
          rotations in each case. In some dimension pairs, the wiggle
          will sweep out many rotations of the circle. In other pairs,
          it will only sweep out a small fraction of a rotation.
          The combination of all these circular wiggles of different
          frequencies gives a good representation of the absolute position
          of a word within the sequence.
        </p>
        <p>
          I'm still developing my intuition for why this works.
          It seems to add position information
          into the mix in a way that doesn't disrupt the learned
          relationships between words and attention.
          For a deeper dive into the math and implications,
          I recommend
          Amirhossein Kazemnejad's positional encoding
          <a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/">
          tutorial</a>.
        </p>
        <p>
          In the canonical architecture diagram these blocks show the
          generation of the position code and its addition to the
          embedded words.
        </p>
        <p style="text-align:center;">
          <img title="Transformer architecture showing positional encoding"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/architecture_positional.png"
            alt="Transformer architecture showing positional encoding"
            style="height:450px;">
        </p>
    ]]></description>
  </item>


  <item>
    <title>
    Beef up security on your self-hosted webserver
    </title>
    <link>
    https://www.brandonrohrer.com/hosting3
    </link>
    <pubDate>
    Sat, 06 Sep 2025 07:34:00 EDT
    </pubDate>
    <guid>
    https://www.brandonrohrer.com/hosting3
    </guid>
    <description><![CDATA[
  <item>
    <title>
    Attaching your web server to a domain name
    </title>
    <link>
    https://www.brandonrohrer.com/hosting2.html
    </link>
    <pubDate>
    Tue, 02 Sep 2025 19:34:00 EDT
    </pubDate>
    <guid>
    https://www.brandonrohrer.com/hosting2.html
    </guid>
    <description><![CDATA[

<p>
In <a href='/hosting'>part one</a> of this series we set up a web server.
The next step is to make it act like a real website.
</p>

<h2><a id="Buy-a-domain-name"></a><a href="#Buy-a-domain-name">Buy a domain name</a></h2>

<p>
The first thing to do is to name it.
Typing <code>http://138.197.69.146</code> to get to your content is impersonal,
boring, and impossible to remember. The name of a website is its
<a href="https://en.wikipedia.org/wiki/Domain_name">domain name</a>.
For example, the domain name for English-language Wikipedia
is <code>en.wikipedia.org</code>.
</p>

<ul>
<li> <code>org</code> is the top level domain (TLD).</li>
<li> <code>wikipedia</code> is the second level domain.</li>
<li> <code>en</code> is a <a href="https://en.wikipedia.org/wiki/Subdomain">subdomain</a>.</li>
</ul>

<p>
Top level domains come in two flavors, generic and country code.
Generic top level domains (gTLDs) include some you are familiar with,
like <code>.com</code> and <code>.org</code>, and a bunch you've probably never seen,
like <code>.anime</code> and <code>.tattoos</code>.
Country code top level domains (ccTLDs) are all two-letter
country codes taken
from <a href="https://en.wikipedia.org/wiki/ISO_3166-1">ISO-3166</a>.
A few of the most common are <code>.uk</code> (United Kingdom),
<code>.de</code> (Germany), and <code>.cn</code> (China).
</p>

<p>
Sometimes people will get creative with country codes and
<a href="https://en.m.wikipedia.org/wiki/Domain_hack">repurpose them</a>.
The <code>.ai</code> (Antigua) ccTLD has been co-opted by a whole generation of
startups. Same for <code>.io</code> (British Indian Ocean Territory).
<code>.me</code> (Montenegro) is a fun one for personalized websites, and the US
state of Maine. <code>.it</code> (Italy) and <code>.to</code> (Tonga) are useful for spelling
English phrases and words like <code>this.is.it</code> and <code>p.ota.to</code>.
</p>

<p>
<figure>
  <img title="A few of the most popular TLDs on Namecheap.  As a rule of thumb, the more popular and commerce-oriented the TLD, the more expensive it will be." alt="Popular TLDs on Namecheap
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/popular_tlds.png">
  <figcaption>A few of the most popular TLDs on Namecheap.  As a rule of thumb, the more popular and commerce-oriented the TLD, the more expensive it will be.</figcaption>
</figure>
</p>

<p>
The <a href="https://data.iana.org/TLD/tlds-alpha-by-domain.txt">comprehesive list of TLDs
</a> has 1439 of them,
as of August 2025 so it's easy to get overwhelmed. If you get paralyzed
by choice, you're always safe going with dot com.
</p>

<p>
Thereâ€™s a running joke that the most important part of any project
is choosing the right domain name. Thereâ€™s another joke that 90% of
projects donâ€™t get past choosing a domain name. Itâ€™s fun but not
critical. Put as much or as little thought into it as you want.
It's OK to make this all about you. And anyway, if insipration strikes
next month you can always buy another domain.
</p>

<p>
There are quite a few reputable domain registrars&mdash;places that can sell you a
domain name&mdash;and it's tough to go wrong. I get mine through
<a href="https://www.namecheap.com/">Namecheap</a>, but other popular ones are
<a href="https://www.hostinger.com">Hostinger</a>,
<a href="https://www.godaddy.com/">GoDaddy</a>, and
<a href="https://porkbun.com/">porkbun</a>.
I found <a href="https://www.techradar.com/news/best-domain-registrars">this TechRadar review
</a>
very helpful. You can go to one of these fine establishments and just
buy your domain name of choice, as long as someone else hasn't beat
you to it. It's so cool that with a few clicks and a few bucks you
can claim a a globally unique piece of real estate and start to build
your digital castle there. Trippy.
</p>

<h2><a id="Point-the-domain-to-your-web-server"></a><a href="#Point-the-domain-to-your-web-server">Point the domain to your web server</a></h2>

<p>
The next step is to inform the domain name servers (DNS) of the world
that your new domain name should point to your web server.
</p>

<p>
This most likely is something you can do through the company that sold you
your domain name. The Namecheap interface looks like this, and other domain
name brokers have a similar page.
</p>

<p>
<img alt="Adding Namecheap DNS records
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/dns_records.png">
</p>

<p>
The important pieces of information here are
</p>

<ul>
<li> <strong>Type</strong> This will be an "A record" (short for <strong>A</strong>dress). There are
a few other useful types but nothing that we need to worry about to get
started. A records establish the IPv4 address assoicated with the domain name.</li>
<li> <strong>Host</strong> This is the domain name being assigned. As a short hand,
<code>@</code> means "whatever domain name you just bought and are now setting up",
maybe something like <code>brandonrohrer.com</code>. <code>www</code> is also shorthand for
<code>www.brandonrohrer.com</code>, just like <code>en</code> in this column would be
shorthand for <code>en.wikipedia.org</code> if we were setting up the Wikipedia page.</li>
<li> ` <strong>Value</strong> The IP address that the Host should be directed to. For us this
will be the IP address of the web server we just created.</li>
<li> <strong>TTL</strong> Time to live for this DNS record on the domain name servers before
they forget about it and have to ask again. "Automatic" is fine unti
you start doing fancy things with DNS.</li>
</ul>

<p>
The entries in this image will send any Internet user in the world who
puts <code>brandonrohrer.com</code> or <code>www.brandonrohrer.com</code> in their browser to
<code>138.197.69.146</code>.
</p>

<p>
DNS is a big topic, and the details can bite you occassionally. If you
have a few minutes and a little curiosity, I recommend checking out
<a href="https://wizardzines.com/zines/dns/">How DNS works</a> by Julia Evans.
It is both ridiculously accessible and deeply informative.
Best $12 you will spend this week.
</p>

<p>
<img alt="The cover of Julia Evans' How DNS works
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/how_dns_works.png">
</p>

<p>
<img alt="Table of contents of How DNS works
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/how_dns_contents.png">
</p>

<h2><a id="Wait"></a><a href="#Wait">Wait</a></h2>

<p>
If your domain is brand new, then the DNS records you added may be available
quickly, but more often it can take up to 48 hours. DNSs hang on to
address records that they've read recently (DNS caching) and only ask
for updates after the TTL has expired. Unfortunately this can be
a couple of days.
</p>

<p>
You can check in on the process while waiting. There are several
command line invocations on Linux and MacOS that resolve a domain name.
After reassigning <code>e2eml.school</code> to <code>138.197.69.146</code> here were some
of the ways it showed up.
</p>

<h3><a id="<code>ping</code>"></a><a href="#<code>ping</code>"><code>ping</code></a></h3>

<p>
<pre>
ping e2eml.school
</pre>
</p>

<p>
gave me
</p>

<p>
<pre>
PING e2eml.school (138.197.69.146): 56 data bytes
64 bytes from 138.197.69.146: icmp_seq=0 ttl=46 time=19.565 ms
64 bytes from 138.197.69.146: icmp_seq=1 ttl=46 time=19.744 ms
64 bytes from 138.197.69.146: icmp_seq=2 ttl=46 time=22.920 ms
64 bytes from 138.197.69.146: icmp_seq=3 ttl=46 time=23.076 ms
...
</pre>
</p>

<p>
<code>ping</code> checks how long it takes to send a packet to the destination and
get a response. In the process it also reports the IP address that
the domain resolved to.
</p>

<h3><a id="<code>host</code>"></a><a href="#<code>host</code>"><code>host</code></a></h3>

<p>
<pre>
host e2eml.school
</pre>
</p>

<p>
gave me
</p>

<p>
<pre>
e2eml.school has address 138.197.69.146
...
</pre>
</p>

<p>
<code>host</code> directly resolves the IP address associated with the domain name.
It doesn't necessarily do this in the same way as <code>ping</code>. It can ask
different computers and programs. So it may give a different answer,
as the DNS record changes roll out to the rest of the DNSs.
</p>

<h3><a id="<code>nslookup</code>"></a><a href="#<code>nslookup</code>"><code>nslookup</code></a></h3>

<p>
<pre>
nslookup e2eml.school
</pre>
</p>

<p>
gave me
</p>

<p>
<pre>
Server:		2001:558:feed::1
Address:	2001:558:feed::1#53 <br>
Non-authoritative answer:
Name:	e2eml.school
Address: 138.197.69.146
</pre>
</p>

<p>
The IP address appears here too. This method is also complementary to
<code>ping</code> and <code>host</code>. It can give a different answer.
</p>

<h3><a id="<code>dig</code>"></a><a href="#<code>dig</code>"><code>dig</code></a></h3>

<p>
<pre>
dig +noall +answer e2eml.school
</pre>
</p>

<p>
gave me
</p>

<p>
<pre>
e2eml.school.		1799	IN	A	138.197.69.146
</pre>
</p>

<p>
In yet another way it returns the IP address for the domain.
</p>

<p>
When all of these agree, that's a pretty good sign that the DNS record change
is complete. But the ultimate test is to run and end to end verification.
Make some change to a <code>.html</code> file on the server and see whether it
shows up when you try to read that file in your browser.
</p>

<h2><a id="Add-your-websites-from-a-repository"></a><a href="#Add-your-websites-from-a-repository">Add your websites from a repository</a></h2>

<p>
You can be done now if you want. You have the <code>http://</code> version
of your domain pointing to your IP address, and a place where you
can add all the html files you like. This next step, while optional,
can make it easier to develop web pages and track your changes.
</p>

<p>
Rather than have your web server be your source of truth for your web pages,
you can have your main copy live in a git repository. Then you just pull
this repository to your web server when you want to roll out edits or
new pages.
</p>

<p>
For convenience, name your git repository the same name as your domain
and put all the files in an <code>html</code> subdirectory.
</p>

<p>
<pre>
my.website.com
â”— html
  â”£ index.html
  â”£ ...
</pre>
</p>

<p>
If you're new to git and want to open the black box, <a href="https://jvns.ca/blog/2024/04/25/new-zine&mdash;how-git-works-/">Julia Evans'
Wizard Zines</a>
is an amazing explainer.
There are
<a href="https://www.brandonrohrer.com/git_resources">some other helpful resources</a>
here, especially <a href="https://ohshitgit.com">Oh shit, git!</a> (
also available <a href="https://wizardzines.com/zines/oh-shit-git/">as a zine</a>
), the
repair kit in case things go off the rails.
</p>

<p>
If your repo is on GitHub, then you can put in to your server where
it needs to be with this:
</p>

<p>
<pre>
cd /var/www/
sudo git clone https://github.com/your-github-username/your-domain.git
</pre>
</p>

<p>
Modify as necessary if you are using another git service.
</p>

<p>
Using the <code>http</code> version works great because the web server will be pull-only
and shouldn't need to mess around with keys or authorization because it's
not going to be making any changes.
And any time you make change to any of your files in the repository, you can
re-deploy it with
</p>

<p>
<pre>
sudo git pull origin HEAD
</pre>
</p>

<h2><a id="Set-up-a-server-block"></a><a href="#Set-up-a-server-block">Set up a server block</a></h2>

<p>
To finish setting up nginx to use your web code from git, follow these
excellent instructions in
<a href="https://www.digitalocean.com/community/tutorials/how-to-install-nginx-on-ubuntu-20-04#step-5-setting-up-server-blocks-recommended">the Digital Ocean docs</a>
that do a better job than I can of walking you through this. There are a lot of
steps, but they are all copy/paste-able into your ssh console.
</p>

<p>
Putting all the files for your domain in its own directory like this referred
to as a server block. While you can just dump them all in the top level
<code>/var/www/html/</code> directory, this gives a little more structure to them.
And the really cool part is that you can repeat this for other
domain names if you want. No need to spin up another server instance
for each one.
</p>

<p>
At this point, you have a fully deployed website at <code>http://your-domain</code>.
The next post is going to change that into a <code>https://</code> and do some
other things to harden the security of your website and server.
It's a brutal world out there, and it pays to protect yourself.
</p>
      ]]></description>
  </item>


  <item>
    <title>
    Transformers from Scratch 10/ðŸ§µ
    </title>
    <link>
    https://www.brandonrohrer.com/transformers#second_order_matrix_mult
    </link>
    <pubDate>
    Tue, 26 Aug 2025 19:34:00 EDT
    </pubDate>
    <guid>
    https://www.brandonrohrer.com/transformers#second_order_matrix_mult
    </guid>
    <description><![CDATA[


        <h3 id="second_order_matrix_mult">
          Second order sequence model as matrix multiplications</h3>
        <p>
          Another step that we have been hand wavy about so far is the
          construction of transition matrices. We have been clear about
          the logic, but not about how to do it with matrix multiplications.
        </p>
        <p>
          Once we have the result of our attention step, a vector
          that includes the most recent word and a small
          collection of the words that have preceded it, we need to
          translate that into features, each of which is a word pair.
          Attention masking gets us the raw material that we need,
          but it doesnâ€™t build those word pair features. To do that,
          we can use a single layer fully connected neural network.
        </p>
        <p>
          To see how a neural network layer can create these pairs,
          we'll hand craft one. It will be artificially clean and stylized,
          and its weights will bear no resemblance to the weights in practice,
          but it will demonstrate how the neural network has the
          expressivity necessary to build these two word pair features.
          To keep it small and clean, will focus on just the three
          attended words from this example,
          <em>battery</em>, <em>program</em>, <em>ran</em>.
        </p>
        <p style="text-align:center;">
          <img title="Neural network layer for creating multi word features"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/feature_creation_layer.png"
            alt="Neural network layer for creating multi word features"
            style="height:350px;">
        </p>
        <p>
          In the layer diagram above, we can see how the weights act to combine
          the presence and absence of each word into a collection of features.
          This can also be expressed in matrix form.
        </p>
        <p style="text-align:center;">
          <img title="Weight matrix for creating multi word features"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/feature_creation_matrix.png"
            alt="Weight matrix for creating multi word features"
            style="height:250px;">
        </p>
        <p>
          And it can be calculated by a matrix multiplication with a
          vector representing the collection of words seen so far.
        </p>
        <p style="text-align:center;">
          <img title="Calculation of the 'battery, ran' feature"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/second_order_feature_battery.png"
            alt="Calculation of the 'battery, ran' feature"
            style="height:200px;">
        </p>
        <p>
          The <em>battery</em> and <em>ran</em> elements are 1 and the
          <em>program</em> element is 0. The <em>bias</em> element is
          always 1, a feature of neural networks. Working through the
          matrix multiplication gives a 1 for the element
          representing <em>battery, ran</em> and a -1 for the element
          representing <em>program, ran</em>. The results for the
          other case are similar.
        </p>
        <p style="text-align:center;">
          <img title="Calculation of the 'program, ran' feature"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/second_order_feature_program.png"
            alt="Calculation of the 'program, ran' feature"
            style="height:200px;">
        </p>
        <p>
          The final step in calculating these word combo features is
          to apply a rectified linear unit (ReLU) nonlinearity. The effect
          of this is to substitute any negative value with a zero. This
          cleans up both of these results so they represent the presence
          (with a 1) or absence (with a 0) of each word combination feature.
        </p>
        <p>
          With those gymnastics behind us, we finally have a matrix
          multiplication based method for creating multiword features.
          Although I originally claimed that these consist of the
          most recent word and one earlier word, a closer look at this
          method shows that it can build other features too.
          When the feature creation matrix is learned, rather than hard
          coded, other structures can be learned. Even in this toy example,
          there's nothing to stop the creation of a three-word combination
          like <em>battery, program, ran</em>. If this combination occurred
          commonly enough it would probably end up being represented.
          There wouldn't be any way to indicated what order the words
          occurred in (at least not <a href="#positional_encoding">yet</a>),
          but we could absolutely use their co-occurrence to make predictions.
          It would even be possible to make use of word combos that ignored
          the most recent word, like <em>battery, program</em>. These and
          other types of features are probably created in practice,
          exposing the oversimiplification I made when I claimed that
          transformers are a selective-second-order-with-skips sequence model.
          There's more nuance to it than that, and now you can see exactly
          what that nuance is. This won't be the last time we'll change
          the story to incorporate more subtlety.
        </p>
        <p>
          In this form, the multiword feature matrix is ready for
          one more matrix multiplication, the
          second order sequence model with skips we developed
          <a href="second_order_skips">above</a>. All together, the
          sequence of
          <ul>
            <li>
              feature creation matrix multiplication,
            </li>
            <li>
              ReLU nonlinearity, and
            </li>
            <li>
              transition matrix multiplication
            </li>
          </ul>
          are the feedforward processing steps that get applied after
          attention is applied.
          Equation 2 from the paper shows these steps
          in a concise mathematical formulation.
        </p>
        <p style="text-align:center;">
          <img title="Equations behind the Feed Forward block"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/feedforward_equations.png"
            alt="Equations behind the Feed Forward block"
            style="height:300px;">
        </p>
        <p>
          The Figure 1 architecture diagram of the of the paper
          shows these lumped together as the Feed Forward block.
        </p>
        <p style="text-align:center;">
          <img title="Transformer architecture showing the Feed Forward block"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/architecture_feedforward.png"
            alt="Transformer architecture showing the Feed Forward block"
            style="height:450px;">
        </p>
      ]]></description>
  </item>

  <item>
    <title>
    Transformers from Scratch 9/ðŸ§µ
    </title>
    <link>
    https://www.brandonrohrer.com/transformers#attention
    </link>
    <pubDate>
    Mon, 25 Aug 2025 19:34:00 EDT
    </pubDate>
    <guid>
    https://www.brandonrohrer.com/transformers#attention
    </guid>
    <description><![CDATA[


        <h3 id="attention">Attention as matrix multiplication</h3>
        <p>
          Feature weights could be straightforward to build by counting
          how often each word pair/next word transition occurs
          in training, but attention masks are not.
          Up to this point, we've pulled the mask vector out of thin air.
          How transformers find the relevant mask matters. It would be
          natural to use some sort of lookup table, but now we are focusing
          hard on expressing everything as matrix multiplications. We can
          use the same <a href="table_lookup">lookup</a> method we
          introduced above by stacking the mask vectors for every word
          into a matrix and using the one-hot representation of the
          most recent word to pull out the relevant mask.
        </p>
        <p style="text-align:center;">
          <img title="Mask lookup by matrix multiplication"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/mask_matrix_lookup.png"
            alt="Mask lookup by matrix multiplication"
            style="height:450px;">
        </p>
        <p>
          In the matrix showing the collection of mask vectors, we've only
          shown the one we're trying to pull out, for clarity.
        </p>
        <p>
          We're finally getting to the point where we can start tying into
          the paper. This mask lookup is represented by the
          <em>QK^T</em> term in the attention equation.
        </p>
        <p style="text-align:center;">
          <img title="Attention equation highlighting QKT"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/attention_equation_QKT.png"
            alt="Attention equation highlighting QKT"
            style="height:100px;">
        </p>
        <p>
          The query <em>Q</em> represents the feature of interest and the
          matrix <em>K</em> represents the collection of masks. Because
          it's stored with masks in columns, rather than rows, it needs to
          be transposed (with the <em>T</em> operator) before multiplying.
          By the time we're all done, we'll make some important modifications
          to this, but at this level it captures the concept of a
          differentiable lookup table that transformers make use of.
        </p>

      ]]></description>
  </item>

  <item>
    <title>
    Transformers from Scratch 8/ðŸ§µ
    </title>
    <link>
    https://www.brandonrohrer.com/transformers#rest_stop
    </link>
    <pubDate>
    Fri, 22 Aug 2025 20:34:00 EDT
    </pubDate>
    <guid>
    https://www.brandonrohrer.com/transformers#rest_stop
    </guid>
    <description><![CDATA[

        <h3 id="rest_stop">Rest Stop and an Off Ramp</h3>
        <p>
          Congratulations on making it this far. You can stop if you want.
          The selective-second-order-with-skips model is a useful way to think
          about what transformers do, at least in the decoder side.
          It captures, to a first approximation, what generative
          language models like
          OpenAI's <a href="https://en.wikipedia.org/wiki/GPT-3">GPT-3</a>
          are doing. It doesn't tell the complete story, but it represents
          the central thrust of it.
        </p>
        <p>
          The next sections cover more of the gap between this
          intuitive explanation and how transformers are implemented.
          These are largely driven by three practical considerations.
          <ol>
            <li>
              <strong>Computers are especially good at matrix
              multiplications.</strong>
              There is an entire industry around
              building computer hardware specifically for fast
              matrix multiplications. Any computation that can
              be expressed as a matrix multiplication can be made
              shockingly efficient. It's a bullet train. If you can get
              your baggage into it, it will get you where you want to go
              real fast.
            </li>
            <li>
              <strong>Each step needs to be differentiable.</strong>
              So far we've just been working with toy examples, and have
              had the luxury of hand-picking all the transition probabilities
              and mask values&mdash;the model <strong>parameters</strong>.
              In practice, these have to be learned via
              <strong>backpropagation</strong>,
              which depends on each computation step
              being differentiable. This means that for any small change
              in a parameter, we can calculate the corresponding
              change in the model error
              or <strong>loss</strong>.
            </li>
            <li>
              <strong>The gradient needs to be smooth and well
              conditioned.</strong>
              The combination of all the
              derivatives for all the parameters is the loss
              <strong>gradient</strong>.
              In practice, getting backpropagation to behave well requires
              gradients that are smooth, that is, the slope doesnâ€™t change
              very quickly as you make small steps in any direction.
              They also behave much better when the gradient is well conditioned,
              that is, itâ€™s not radically larger in one direction than another.
              If you picture a loss function as a landscape, The Grand Canyon
              would be a poorly conditioned one. Depending on whether you are
              traveling along the bottom, or up the side, you will have very
              different slopes to travel. By contrast, the rolling hills of
              the classic Windows screensaver would have a well conditioned
              gradient.
              <br>
              If the science of architecting neural networks is creating
              differentiable building blocks, the art of them is stacking
              the pieces in such a way that
              the gradient doesnâ€™t change too quickly
              and is roughly of the same magnitude in every direction.
            </li>
          </ol>
        </p>

      ]]></description>
  </item>

  <item>
    <title>
    Transformers from Scratch 7/ðŸ§µ
    </title>
    <link>
    https://www.brandonrohrer.com/transformers#masking
    </link>
    <pubDate>
    Thu, 21 Aug 2025 20:34:00 EDT
    </pubDate>
    <guid>
    https://www.brandonrohrer.com/transformers#masking
    </guid>
    <description><![CDATA[

        <h3 id="masking">Masking</h3>
        <p>
          On more careful consideration, this is unsatisfying. The difference
          between a vote total of 4 and 5 is relatively small. It suggests
          that the model isn't as confident as it could be. And in
          a larger, more organic language model it's easy to imagine that
          such a slight difference could be lost in the statistical noise.
        </p>
        <p>
          We can sharpen the prediction by weeding out all the uninformative
          feature votes. With the exception of <em>battery, ran</em>
          and <em>program, ran</em>. It's helpful to remember at this point
          that we pull the <a href="#table_lookup">relevant rows</a>
          out of the transition matrix by
          multiplying it with a vector showing which features are currently
          active. For this example so far, we've been using the implied
          feature vector shown here.
        </p>
        <p style="text-align:center;">
          <img title="Feature selection vector"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/feature_selection.png"
            alt="Feature selection vector"
            style="height: 150px;">
        </p>
        <p>
          It includes a one for each feature that is a combination of
          <em>ran</em> with each of the words that come before it. Any words
          that come after it don't get included in the feature set.
          (In the next word prediction problem these haven't been seen yet,
          and so it's not fair to use them predict what comes next.)
          And this doesn't include all the other possible word
          combinations.
          We can safely ignore these for this example because they will
          all be zero.
        </p>
        <p>
          To improve our results, we can additionally force the unhelpful
          features to zero by creating a <strong>mask</strong>. It's
          a vector full of ones except for the positions you'd like to
          hide or mask, and those are set to zero. In our case we'd like
          to mask everything except for 
          <em>battery, ran</em>
          and <em>program, ran</em>, the only two features that have
          been of any help. 
        </p>
        <p style="text-align:center;">
          <img title="Masked feature activities"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/masked_feature_activities.png"
            alt="Masked feature activities"
            style="height:300px;">
        </p>
        <p>
          To apply the mask, we multiply the two vectors element by element.
          Any feature activity value in an unmasked position will be
          multiplied by one and left unchanged. Any feature activity value
          in a masked position will be multiplied by zero, and thus forced
          to zero.
        </p>
        <p>
          The mask has the effect of hiding a lot of the transition matrix.
          It hides the combination of <em>ran</em> with everything except
          <em>battery</em> and <em>program</em>, leaving just the features
          that matter. 
        </p>
        <p style="text-align:center;">
          <img title="Masked transition matrix"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/masked_transition_matrix.png"
            alt="Masked transition matrix"
            style="height:350px;">
        </p>
        <p>
          After masking the unhelpful features, the next word predictions
          become much stronger. When the word <em>battery</em> occurs
          earlier in the sentence, the word after <em>ran</em> is
          predicted to be <em>down</em> with a weight of 1 and
          <em>please</em> with a weight of 0. What was a weight difference
          of 25 percent has become a difference of infinity percent.
          There is no doubt what word comes next. The same strong
          prediction occurs for <em>please</em> when
          <em>program</em> occurs early on.
        </p>
        <p>
          This process of selective masking is the <strong>attention</strong>
          called out in the title of the original
          <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">
            paper</a> on transformers.
          So far, what we've descibed is a just an approximation of
          how attention is implemented in the paper. It captures
          the important concepts, but the details are different. We'll
          close that gap later.
        </p>

      ]]></description>
  </item>

  <item>
    <title>
    Transformers from Scratch 6/ðŸ§µ
    </title>
    <link>
    https://www.brandonrohrer.com/transformers#second_order_skips
    </link>
    <pubDate>
    Wed, 20 Aug 2025 06:34:00 EDT
    </pubDate>
    <guid>
    https://www.brandonrohrer.com/transformers#second_order_skips
    </guid>
    <description><![CDATA[
      <h3 id="second_order_skips">Second order sequence model with skips</h3>
        <p>
          A second order model works well when we only have to look back
          two words to decide what word comes next. What about when we
          have to look back further? Imagine we are building yet another
          language model. This one only has to represent two sentences,
          each equally likely to occur.
          <ul>
            <li>
              <em>Check the program log and find out whether it ran please.
              </em>
            </li>
            <li>
              <em>Check the battery log and find out whether it ran down
              please.</em>
            </li>
          </ul>
        </p>
        <p>
          In this example, in order to determine which word should come after
          <em>ran</em>, we would have to look back 8 words into the past.
          If we want to improve on our second order language model,
          we can of course consider third- and higher order models.
          However, with a significant vocabulary size this takes
          a combination of creativity and brute force
          to execute. A naive implementation of an eighth order model
          would have <em>N</em>^8 rows, a ridiculous number for any
          reasonable vocubulary.
        </p>
        <p>
          Instead, we can do something sly and make a second order model,
          but consider the combinations of the most recent word with
          each of the words that came before. It's still second order,
          because we're only considering two words at a time, but it allows
          us to reach back further and capture <strong>long range
          dependencies</strong>. The difference between this
          second-order-with-skips and a full umpteenth-order model is that
          we discard most of the word order information and
          combinations of preceeeding words. What remains is still pretty
          powerful.
        </p>
        <p>
          Markov chains fail us entirely now, but we can still represent
          the link between each pair of preceding words and the words
          that follow. Here we've dispensed with numerical weights, and
          instead are showing only the arrows associated with non-zero weights.
          Larger weights are shown with heavier lines.
        </p>
        <p style="text-align:center;">
          <img title="Second order with skips feature voting"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/feature_voting.png"
            alt="Second order with skips feature voting"
            style="height: 350px;">
        </p>
        <p>
          Here's what it might look like in a transition matrix.
        </p>
        <p style="text-align:center;">
          <img title="Second order with skips transition matrix"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/transition_matrix_second_order_skips.png"
            alt="Second order with skips transition matrix"
            style="height: 350px;">
        </p>
        <p>
          This view only shows the rows relevant to predicting the word
          that comes after <em>ran</em>. It shows instances where the most
          recent word (<em>ran</em>) is preceded by each of the other
          words in the vocabulary. Only the relevant values are shown.
          All the empty cells are zeros. 
        </p>
        <p>
          The first thing that becomes apparent is that, when trying to
          predict the word that comes after <em>ran</em>, we no longer
          look at just one line, but rather a whole set of them.
          We've moved out of the Markov realm now. Each row no longer
          represents the state of the sequence at a particular point.
          Instead, each row represents one of many <strong>features</strong>
          that may describe the sequence at a particular point. The
          combination of the most recent word with each of the words
          that came before makes for a collection of applicable rows,
          maybe a large collection. Because of this change in meaning,
          each value in the matrix no longer represents a probability,
          but rather a vote. Votes will be summed and compared to determine
          next word predictions.
        </p>
        <p>
          The next thing that becomes apparent is that most of the features
          don't matter. Most of the words appear in both sentences, and
          so the fact that they have been seen is of no help in predicting
          what comes next. They all have a value of .5. 
          The only two exceptions are <em>battery</em> and <em>program</em>.
          They have some 1 and 0 weights associated with
          the two cases we're trying to distinguish.
          The feature <em>battery, ran</em> indicates that <em>ran</em> was
          the most recent word and that <em>battery</em> occurred somewhere
          earlier in the sentence. This feature has a weight of 1 associated
          with <em>down</em> and a weight of 0 associated with <em>please</em>.
          Similarly, the feature <em>program, ran</em> has the opposite set
          of weights. This structure shows that it is the presence of these
          two words earlier in the sentence that is decisive in predicting
          which word comes next.
        </p>
        <p>
          To convert this set of word-pair features into a next word estimate,
          the values of all the relevant rows need to be summed.
          Adding down the column, the sequence
          <em>Check the program log and find out whether it ran</em>
          generates sums of 0 for all the words, except a 4 for
          <em>down</em> and a 5 for <em>please</em>. The sequence
          <em>Check the battery log and find out whether it ran</em>
          does the same, except with a 5 for
          <em>down</em> and a 4 for <em>please</em>. By choosing the word
          with the highest vote total as the next word prediction,
          this model gets us the right answer, despite having an
          eight word deep dependency.
        </p>

      ]]></description>
  </item>

  <item>
    <title>
    Transformers from Scratch 5/ðŸ§µ
    </title>
    <link>
    https://www.brandonrohrer.com/transformers#second_order
    </link>
    <pubDate>
    Tue, 19 Aug 2025 20:34:00 EDT
    </pubDate>
    <guid>
    https://www.brandonrohrer.com/transformers#second_order
    </guid>
    <description><![CDATA[
        <h3 id="second_order">Second order sequence model</h3>
        <p>
          Predicting the next word based on only the current word is
          hard. That's like predicting the rest of a tune after being
          given just the first note. Our chances are a lot better if
          we can at least get two notes to go on.
        </p>
        <p>
          We can see how this works in another toy language model
          for our computer commands. We expect that this one
          will only ever see two sentences, in a 40/60 proportion.
          <ul>
            <li>
              <em>Check whether the battery ran down please.</em>
            </li>
            <li>
              <em>Check whether the program ran please.</em>
            </li>
          </ul>
          A Markov chain illustrates a first order model for this.
        </p>
        <p style="text-align:center;">
          <img title="Another first order Markov chain transition model"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/markov_chain_2.png"
            alt="Another first order Markov chain transition model"
            style="height: 250px;">
        </p>
        <p>
          Here we can see that if our model looked at the two most recent
          words, instead of just one, that it could do a better job. When it
          encounters <em>battery ran</em>, it knows that the next word
          will be <em>down</em>, and when it sees <em>program ran</em>
          the next word will be <em>please</em>. This eliminates one of
          the branches in the model, reducing uncertainty and increasing
          confidence.
          Looking back two words turns this into a second order Markov model.
          It gives more context on which to base next word predictions.
          Second order Markov chains are more challenging to
          draw, but here are the connections that demonstrate their value.
        </p>
        <p style="text-align:center;">
          <img title="Second order Markov chain"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/markov_chain_second_order.png"
            alt="Second order Markov chain"
            style="height:250px;">
        </p>
        <p>
          To highlight the difference between the two,
          here is the first order transition matrix,
        </p>
        <p style="text-align:center;">
          <img title="Another first order transition matrix"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/transition_matrix_first_order_2.png"
            alt="Another first order transition matrix"
            style="height: 350px;">
        </p>
        <p>
          and here is the second order transition matrix.
        </p>
        <p style="text-align:center;">
          <img title="Second order transition matrix"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/transition_matrix_second_order.png"
            alt="Second order transition matrix"
            style="height: 350px;">
        </p>
        <p>
          Notice how the second order matrix has a separate row for every
          combination of words (most of which are not shown here). That
          means that if we start with a vocabulary size of <em>N</em>
          then the transition matrix has <em>N</em>^2 rows.
        </p>
        <p>
          What this
          buys us is more confidence. There are more ones and fewer
          fractions in the second order model. There's only one row
          with fractions in it, one branch in our model. Intuitively,
          looking at two words instead of just one gives more context,
          more information on which to base a next word guess.
        </p>

      ]]></description>
  </item>

  <item>
    <title>
    How to start publishing an RSS feed
    </title>
    <link>
    https://www.brandonrohrer.com/rss.html
    </link>
    <pubDate>
    Mon, 18 Aug 2025 20:34:00 EDT
    </pubDate>
    <guid>
    https://www.brandonrohrer.com/rss.html
    </guid>
    <description><![CDATA[
<p>
Before Twitter, before LinkedIn, before Facebook, there was
<a href="https://en.wikipedia.org/wiki/RSS">RSS</a>.
Really Simple Syndication is the photocopied 'zine of microblogging.
If there were social media in Mad Max, it would have been RSS. It's
totally outside any centralized control, cheap, gritty, and punk af. 
</p>

<h2><a id="Reading-RSS"></a><a href="#Reading-RSS">Reading RSS</a></h2>

<p>
The toughest thing to get used to is that there is no central platform.
RSS is just a bunch of people creating RSS-formatted files and posting them
on the internet. The burden of assembling them into a reading list falls
on the reader. Thankfully there <strong>aggregators</strong>, helpful programs that regularly
check those RSS files for changes and lay them out for you in a feed.
</p>

<p>
I use <a href="https://feedly.com">Feedly</a> daily, and enjoy
<a href="https://www.inoreader.com">Inoreader</a> too. I've also heard that 
<a href="https://feeder.co">Feeder</a> and 
<a href="https://newsblur.com">NewsBlur</a> are solid options. There are plenty
of others, some with niche functionality. The cool part is that there isn't
a "main" one or a home base. They're all their own thing. All they do
is gather up changes and put them in a list for you.
</p>

<p>
To subscribe to someone's feed, you'll need to get the URL. These are
frequently posted on their blog under an "RSS feed" link or the
RSS logo.
</p>

<p>
<img alt="The RSS logo, a dot at the center of two quarter-circles, 
giving the appearance of waves radiating out from a central point
" src="https://upload.wikimedia.org/wikipedia/en/4/43/Feed-icon.svg">
</p>

<p>
The URL is to an .xml file. For my blog it looks like
</p>

<p>
<pre>
https://brandonrohrer.com/feed.xml
</pre>
</p>

<p>
For an example feed created for this post, it looks like
</p>

<p>
<pre>
https://raw.githubusercontent.com/brohrer/blog/refs/heads/main/code/example_feed.xml
</pre>
</p>

<p>
This is the key to subscribing. Copy the URL and paste it into the
field for 
"Add channel" or "Follow feeds" or whatever other name your aggregator uses.
And from then on your aggregator will revisit that URL occasionally,
checking for new content, and add it to your feed.
</p>

<p>
<figure>
  <img title="A Follow Sources dialog from Feedly" alt="A Follow Sources dialog page, with a field for pasting a feed URL
and below that a list of feeds that match the URL
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/rss/follow_sources.png">
  <figcaption>A Follow Sources dialog from Feedly</figcaption>
</figure>
</p>

<p>
<figure>
  <img title="A channel overview on Feedly" alt="An overview of the feed, showing channel title and a thumbnail
view of the first post
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/rss/feed_overview.png">
  <figcaption>A channel overview on Feedly</figcaption>
</figure>
</p>

<p>
<figure>
  <img title="An example post, viewed on Feedly" alt="An example post with the title Post Content and some text reading 
This can be any html.
followed by a vintage drawing of Christopher Robin reading to 
Winnie the Pooh, who is stuck in a hole.
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/rss/example_post.png">
  <figcaption>An example post, viewed on Feedly</figcaption>
</figure>
</p>

<p>
Most aggregators also have a way to explore the more popular feeds.
</p>

<p>
<figure>
  <img title="Some popular feed topics, offered on Feedly" alt="A list of 15 industries and a handful skills as topics to search for feeds
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/rss/feed_topics.png">
  <figcaption>Some popular feed topics, offered on Feedly</figcaption>
</figure>
</p>

<h2><a id="Writing-RSS"></a><a href="#Writing-RSS">Writing RSS</a></h2>

<p>
If the process of following RSS feeds seems duct-tape-and-bailing-wire,
you'll love writing them.
</p>

<h3><a id="1.-Find-a-place-to-host-it"></a><a href="#1.-Find-a-place-to-host-it">1. Find a place to host it</a></h3>

<p>
Aggregators need to be able to find your feed .xml file on the internet.
Any place you can upload a text file and share it with the world should do.
One free option is <a href="https://www.github.com">GitHub</a> if that's a tool
you're familiar with. Here's 
<a href="https://github.com/brohrer/blog/blob/main/code/example_feed.xml">the .xml file for the example feed
</a>
which is on GitHub.
</p>

<p>
<pre>
&lt;rss version="2.0"&gt;
  &lt;channel&gt;
    &lt;title&gt;Your Feed&lt;/title&gt;
    &lt;link&gt;https://www.brandonrohrer.com&lt;/link&gt;
    &lt;description&gt;Your Blog's name&lt;/description&gt; <br>
    &lt;item&gt;
      &lt;title&gt;Blog title&lt;/title&gt;
      &lt;link&gt;https://www.brandonrohrer.com/rss.html&lt;/link&gt;
      &lt;pubDate&gt;Sat, 16 Aug 2025 12:31:00 EDT&lt;/pubDate&gt;
      &lt;guid isPermaLink="false"&gt;https://www.brandonrohrer.com/rss.html.04&lt;/guid&gt;
      &lt;description&gt;&lt;![CDATA[
        &lt;h1&gt;Post content&lt;/h1&gt;
        &lt;p&gt;
          This can be any html.
        &lt;/p&gt;
        &lt;img src="https://upload.wikimedia.org/wikipedia/commons/9/97/Winnie-the-Pooh_45-1.png"&gt;
      ]]&gt;&lt;/description&gt;
    &lt;/item&gt; <br>
  &lt;/channel&gt;
&lt;/rss&gt;
</pre>
</p>

<p>
You can copy this directly into your own feed .xml and modify it. 
A trick to remember with GitHub is that the link to your feed will actually
be the "raw" link, which is available from the icon on the right side
of the screen when looking at the file in GitHub.
</p>

<p>
There are two major sections, the <code>channel</code> information at the top,
then information for each <code>item</code> below that.
</p>

<h3><a id="2.-Add-channel-information"></a><a href="#2.-Add-channel-information">2. Add channel information</a></h3>

<ul>
<li> <code>title</code> is the channel name. It can be anything you want. Itâ€™s what
people will see when they pull up your channel in their aggregator.</li>
<li> <code>link</code> is a website associated with your channel. For me, it's the
landing page of my blog.</li>
<li> <code>description</code> is typically a one-line explanation of what readers can
expect to see in your feed.</li>
</ul>

<p>
There are <a href="https://www.rssboard.org/rss-specification#optionalChannelElements">lots of other elements
</a>
you can add here if you like, but these are the required ones.
</p>

<h3><a id="3.-Add-item-information"></a><a href="#3.-Add-item-information">3. Add item information</a></h3>

<p>
Once you have the channel info in place you can add an item.
An item needs a few basic pieces of information.
</p>

<ul>
<li> <code>title</code> is the name of the particular post. </li>
<li> <code>link</code> is a URL associated with it.</li>
<li> <code>pubDate</code> is <a href="https://whitep4nth3r.com/blog/how-to-format-dates-for-rss-feeds-rfc-822/#valid-rfc-822-date-format">a date in the format of
</a>
<code>Fri, 16 Aug 2025 19:31:00 EDT</code>. It shows up at the top of a post as
the publication date.</li>
<li> <code>guid</code> (globally unique identifier) is a string that aggregators can use
as an ID for this post. I find it useful for when I update the content
of the post and I want aggregators to re-load it on their next pass.
Changing the guid signals to the aggregator that the post needs to be
re-loaded. By default, RSS assumes that this is a permanent link to the
website being promoted by this post item. If you include the argument
<code>isPermaLink="false"</code> that signals that this isn't intended to be a URL, just
an identifying string.</li>
<li> <code>description</code> is the body of the post. It can be a one-line teaser
for the linked content or it can be an entire novel. Everything
between the <code>&lt;![CDATA[</code> and <code>]]&gt;</code> delimiters will be interpreted as straight
html, which is super useful if you want to do pretty formatting or
include images. Not all aggregators will interpret all html tags
(for instance <code>&lt;script&gt;</code>
<a href="https://validator.w3.org/feed/docs/warning/SecurityRisk.html">is likely to get skipped</a>
for security reasons),
but any html that gives the aggregator pause will usually just be skipped over.</li>
</ul>

<p>
There are a number of
<a href="https://www.rssboard.org/rss-specification#hrelementsOfLtitemgt">other item elements</a>
you an add if you wish, but this is the subset I've found most useful.
</p>

<p>
You can add as many items as you like. Just repeat the <code>&lt;item&gt;</code> section.
</p>

<p>
<pre>...
    &lt;item&gt;
      &lt;title&gt;First post&lt;/title&gt;
      ...
    &lt;/item&gt;
    ...
    &lt;item&gt;
      &lt;title&gt;Second post&lt;/title&gt;
      ...
    &lt;/item&gt;
   ...
    &lt;item&gt;
      &lt;title&gt;Third post&lt;/title&gt;
      ...
    &lt;/item&gt;
    ...
</pre>
</p>

<p>
And that's it. You've created an RSS feed. Every time you edit your .xml
file, you've updated your feed.
</p>

<p>
Debugging feed .xml files can be fiddly. There are a lot of details that
have to be just so, and it can take a loooong time to
wait for the aggregrator to re-scan so that you can check the results.
A cool thing I discovered while writing this is that there are RSS
validation services like 
<a href="https://validator.w3.org/feed/">this one</a> and
<a href="https://www.rssboard.org/rss-validator/check.cgi">this one</a>
where you can
enter the contents of your feed file or the URL for your feed,
and it will check your feed for errors
right away. It revealed several imperfections in my own feed that I 
was oblivious to.
</p>

<h2><a id="RSS-is-different-than-other-social-media"></a><a href="#RSS-is-different-than-other-social-media">RSS is different than other social media</a></h2>

<h3><a id="Things-that-you-give-up"></a><a href="#Things-that-you-give-up">Things that you give up</a></h3>

<p>
When you send an RSS post out into the Internet, you can't tell whether one
person sees it or a thousand. You have no <strong>analytics</strong>, no likes, hearts,
or stars, no engagement dopamine. There is also no <strong>interaction</strong> with your
your readers, no comment section, Q and A, or quote posts.
There is also no <strong>algorithm</strong> you can game for boosting. Posts
just show up in the order they are published.
</p>

<h3><a id="Things-you-get"></a><a href="#Things-you-get">Things you get</a></h3>

<p>
In exchange for all that, you get a more <strong>deliberate experience</strong> of reading and writing.
There is more friction in publishing and following a feed than of downloading
an app and snapping a picture of your latte.
RSS requires intention, and so invites more thought.
It makes doomscrolling all but impossible.
Related to this, posts are often longer and more thought out than
the Tweets of yore. In my experience there are
<strong>more quirks and rabbit holes</strong>.
Most importantly, you also get <strong>control</strong> over what you can publish and
in what format.  You have independence from any rules or policy decisions
that might be imposed by a central authority.
There is no one who can boost it, bury it, or shut it down.
It is an indestructible distribution channel.
</p>

<h3><a id="A-note-about-Atom"></a><a href="#A-note-about-Atom">A note about Atom</a></h3>

<p>
You don't have to spend long in the world of RSS before you start hearing about
Atom. Atom is a very similar format that serves a very similar purpose.
It was backed by Google and for a while there was a great deal of contention
between proponents of RSS and Atom, which led to a lot of intense...um...discussion. 
<a href="https://danielmiessler.com/blog/atom-rss-why-we-should-just-call-them-feeds-instead-of-rss-feeds">Atom is better</a>
in the sense that it resolved several points of ambiguous behavior and
unappealing flaws of RSS, but in retrospect, the "reader wars" were probably
more distracting than productive and left the community a little fragmented.
</p>

<p>
Atom still exists. Plenty of people still use it, although from what
I gather RSS is more popular. Most aggregators can read both RSS and Atom just fine.
The shortcomings of RSS are not anything that is likely to bother any but
the geekiest users. I leave it as an exercise to the reader to decide if it
matters enough to spend any cognitive cycles on.
</p>

      ]]></description>
  </item>

  <item>
    <title>
    Transformers from Scratch 4/ðŸ§µ
    </title>
    <link>
    https://www.brandonrohrer.com/transformers#markov_chain
    </link>
    <pubDate>
    Fri, 15 Aug 2025 19:01:00 EDT
    </pubDate>
    <guid isPermaLink="false">
    2025081502
    </guid>
    <description><![CDATA[
        <h3>First order sequence model</h3>
        <p>
          We can set aside matrices for a minute and get back to what
          we really care about, sequences of words. Imagine that as we
          start to develop our natural language computer interface
          we want to handle just three possible commands:
        </p>
          <ul>
            <li>
              <em>Show me my directories please</em>.
            </li>
            <li>
              <em>Show me my files please</em>.
            </li>
            <li>
              <em>Show me my photos please</em>.
            </li>
          </ul>
        <p>
          Our vocabulary size is now seven:<br>
          {<em>directories, files, me, my, photos, please, show</em>}.
        </p>
        <p>
          One useful way to represent sequences is with a transition model.
          For every word in the vocabulary, it shows what the next word
          is likely to be. If users ask about photos half
          the time, files 30% of the time, and directories the rest
          of the time, the transition model will look like this.
          The sum of the transitions away from any word will always add up
          to one.
        </p>
          <img title="Markov chain transition model"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/markov_chain.png"
            alt="Markov chain transition model"
            style="height: 250px;">
        <p>
          This particular transition model is called a
          <strong>Markov chain</strong>,
          because it satisfies the
          <a href="https://en.wikipedia.org/wiki/Markov_property">
            Markov property</a>
          that the probabilities for the next word depend only on
          recent words. More specifically, it is a first order
          Markov model because it only looks at the single most recent word.
          If it considered the two most recent words it would be
          a second order Markov model.
        </p>
        <p>
          Our break from matrices is over. It turns out that
          Markov chains can be expressed conveniently in matrix form.
          Using the same indexing scheme that we used when creating one-hot
          vectors, each row represents one of the words in our vocabulary.
          So does each column. The matrix transition model
          treats a matrix as a lookup table. Find the row
          that corresponds to the word youâ€™re interested in. The value in
          each column shows the probability of that word coming next.
          Because the value of each element in the matrix represents
          a probability, they will all fall between zero and one.
          Because probabilities always sum to one, the values in each
          row will always add up to one.
        </p>
          <img title="Transition matrix"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/transition_matrix.png"
            alt="Transition matrix"
            style="height: 300px;">
        <p>
          In the transition matrix here we can see the structure
          of our three sentences clearly. Almost all of the transition
          probabilities are zero or one. There is only one place in
          the Markov chain where branching happens. After <em>my</em>,
          the words <em>directories</em>, <em>files</em>, or <em>photos</em>
          might appear, each with a different probability. Other than that,
          thereâ€™s no uncertainty about which word will come next.
          That certainty is reflected by having mostly ones and zeros in the
          transition matrix.
        </p>
        <p>
          We can revisit our trick of using matrix multiplication
          with a one-hot vector to pull out the transition probabilities
          associated with any given word. For instance, if we just wanted
          to isolate the probabilities of which word comes after <em>my</em>,
          we can create a one-hot vector representing the word <em>my</em>
          and multiply it by our transition matrix. This pulls out
          the relevant row and shows us the probability distribution
          of what the next word will be.
        </p>
          <img title="Transition probability lookup"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/transition_lookups.png"
            alt="Transition probability lookup"
            style="height: 300px;">
      ]]></description>
  </item>


  <item>
    <title>
    Transformers from Scratch 3/ðŸ§µ
    </title>
    <link>
    https://www.brandonrohrer.com/transformers
    </link>
    <pubDate>
    Fri, 15 Aug 2025 07:01:00 EDT
    </pubDate>
    <guid isPermaLink="false">
    2025081500
    </guid>
    <description><![CDATA[
        <h3 id="matrix_multiplication">Matrix multiplication</h3>
        <p>
          The dot product is the building block of matrix multiplication,
          a very particular way to combine a pair of two-dimensional arrays.
          We'll call the first of these matrices <em>A</em> and the second
          one <em>B</em>.
          In the simplest case, when <em>A</em> has only one row and
          <em>B</em> has only one column, the result of matrix multiplication
          is the dot product of the two.
        </p>
        <p style="text-align:center;">
          <img title="multiplication of a single row matrix and a single column matrix"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/matrix_mult_one_row_one_col.png"
            alt="multiplication of a single row matrix and a single column matrix"
            style="height: 300px;">
        </p>
        <p>
          Notice how the number of columns in <em>A</em> and the number of
          rows in <em>B</em> needs to be the same for the two arrays
          to match up and for the dot product to work out.
        </p>
        <p>
          When <em>A</em> and <em>B</em> start to grow, matrix multiplication
          starts to get trippy. To handle more than one row in <em>A</em>,
          take the dot product of <em>B</em> with each row separately.
          The answer will have as many rows as <em>A</em> does.
        </p>
        <p style="text-align:center;">
          <img title="multiplication of a two row matrix and a single column matrix"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/matrix_mult_two_row_one_col.png"
            alt="multiplication of a two row matrix and a single column matrix"
            style="height: 250px;">
        </p>
        <p>
          When <em>B</em> takes on more columns, take the dot product of
          each column with <em>A</em> and stack the results in successive
          columns.
        </p>
        <p style="text-align:center;">
          <img title="multiplication of a one row matrix and a two column matrix"
            src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/transformers/matrix_mult_one_row_two_col.png"
